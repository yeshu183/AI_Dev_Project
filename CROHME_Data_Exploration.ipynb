{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11563022,"sourceType":"datasetVersion","datasetId":7250052},{"sourceId":11571455,"sourceType":"datasetVersion","datasetId":7254592}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom tqdm import tqdm\nimport re\nimport time\nimport argparse","metadata":{"_uuid":"7759b3bc-8050-4539-96b9-60c6903995e4","_cell_guid":"17a16fb5-f86c-4ff3-8932-b68854725d90","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-26T03:44:19.491972Z","iopub.execute_input":"2025-04-26T03:44:19.492181Z","iopub.status.idle":"2025-04-26T03:44:19.503373Z","shell.execute_reply.started":"2025-04-26T03:44:19.492153Z","shell.execute_reply":"2025-04-26T03:44:19.502591Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:19.503948Z","iopub.execute_input":"2025-04-26T03:44:19.504132Z","iopub.status.idle":"2025-04-26T03:44:27.217420Z","shell.execute_reply.started":"2025-04-26T03:44:19.504116Z","shell.execute_reply":"2025-04-26T03:44:27.216857Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_path = \"/kaggle/input/crohme-data-basic/filtered_basic_arithmetic/train\"\nimg_path = train_path+\"/images/expr_\"+\"009203.png\"\nprint(img_path)\nimg = Image.open(img_path)\nplt.imshow(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.218753Z","iopub.execute_input":"2025-04-26T03:44:27.219082Z","iopub.status.idle":"2025-04-26T03:44:27.485351Z","shell.execute_reply.started":"2025-04-26T03:44:27.219063Z","shell.execute_reply":"2025-04-26T03:44:27.484519Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/crohme-data-basic/filtered_basic_arithmetic/train/images/expr_009203.png\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x79085f68eb90>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAa4AAAGiCAYAAAC/NyLhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQdElEQVR4nO3deVhUZfsH8O+ZhWEdFtkFFAVccUVx1MyCxLVF2y2XfDUVK7PM136m2VtZ1ltWbu22WFlZWlamicurIu6pqCiI4gKIyL4Ms5zfH+TgMIOKwqzfz3VxXZ7nOTNzzxHmnuc5z7mPIIqiCCIiIjshsXYAREREjcHERUREdoWJi4iI7AoTFxER2RUmLiIisitMXEREZFeYuIiIyK4wcRERkV1h4iIiIrvCxEVERHbFaolryZIlaN26NVxdXREfH4/du3dbKxQiIrIjVklcq1atwowZMzBv3jzs378fXbt2RVJSEi5evGiNcIiIyI4I1iiyGx8fj169emHx4sUAAL1ej/DwcDz11FP497//belwiIjIjsgs/YI1NTXYt28fZs+ebWiTSCRITExEamqq2ceo1Wqo1WrDtl6vx+XLl9GiRQsIgtDsMRMRUdMSRRFlZWUIDQ2FRNK4yT+LJ65Lly5Bp9MhKCjIqD0oKAjHjx83+5gFCxZg/vz5lgiPiIgs6OzZswgLC2vUYyyeuG7G7NmzMWPGDMN2SUkJIiIicGZ/ayg9uTCSiMjelJbr0arHaXh5eTX6sRZPXP7+/pBKpcjPzzdqz8/PR3BwsNnHKBQKKBQKk3alpwRKLyYuIiJ7dTOneyz+qe/i4oKePXti06ZNhja9Xo9NmzZBpVJZOhwiIrIzVpkqnDFjBsaOHYu4uDj07t0bixYtQkVFBcaPH2+NcIiIyI5YJXE99NBDKCgowNy5c5GXl4du3bph/fr1Jgs2iIiI6rPKdVy3qrS0FN7e3ig60YbnuIiI7FBpmR6+MadQUlICpVLZqMfyU5+IiOyKXSyHJ2oKOlEPLXTN+hoKQd6sz09ETFzkRBKP3ge36a7N+hp3rNqLmX5ZzfoaRM6OiYvs3sy87vjxQM/r7ud90AUu6TubNZZPfxqEpS01Rm0ShQ5/D1wGT0nzJk0iZ8HERXYhV1uOygaWEf2cEo+Y53dZNqAGtJprWm9TGhCAtF0eaC0rMbSFSF3gLnGxZGhEDoOJi+zCiPkzEfhLptm+6IpD0Fs4nsbQFRTgv/EDAeGqtVDfy/F7u9+tFhORPWPiIptyf1YijqTEmLS3Sb0EXUHBTT/v6f+oUBOiuf6Ot6jDwiLoTpie49JdKjTaLlqhQvs2Uw3bstgSHOmzstnjI3IETFxkVVuqJCjWuxu2D2+JRut5puehrrUWUOrfAghscc3XmTbydzzle+Zmw7xh8VunoIVMatyo09cms6sumfT9IhW+V+1SMroP1sR6AgCi5QXo5OLW7LES2StegExWdddD4yFNPWzYFnU6ow/4G3HqDRWOPP7+Nfex1DJ1tWg6qjtSI+KlHknQFRU1/EBBgCCtTXjZL/dCxhPLmitEIptwKxcgc8RFFtF203j4bjVdVRd47AR0Wu11H5/1lgpBsflm+5LD/7CZ66fMxdFOXo2q75Wo0dWNsRRv+0L+1766nUQR4j/HIXJ1KeJOTwEABI4+w3NhRPUwcVGz2VApx8bSzgAA/42u8PnSdMVd/SlAaVQkqtuYTvsNv3MPFoXsbY4wm52nxBWbO601ams3cApaSuIM24qLldAfPAoAEA+ko8WB2vZTESrM9O4OABjnt5NTiETgVCE1o7arJiPq2cYtU896S4XM0c43Tab6exSUQ6594XLVn5HYFvuzhSIial6cKiSb0P21qfDJqjvH0+5UwXULLOWt6YA7wk8atp/2/ryZorNtH3ZYic/29QcA/Lm2NyJeMV2g4v6iO+4ImGjYPjdQhpNjnC/JEzFx0S2Zer4PjhYFQy8KCPnrInQZdddaXZ20dAN7oCSy3l2sBeA/nb7A3R6VlgnWhnVxcTVMhSYMCMXl8XU3VZVXivD6Pg3ivnRcfclyS6EXBva4FwAwLPQwS02R0+BUITXa1cVqE56eBo/VaaY7SaQQJHW35D7zbQcc6/eVpUJ0KKvLlfikS0foq6sb3Cdnbl8cevIDyAVpg/sQ2RJOFZJFdfgiGW2/q13a7ZV91KRqhcTVFV1Sq9Hdve66qXjXLQA8LRajIxnifgm6Q8ew6KVH4LXK/DnDyGUnMTRlAj5auRiRch5ncmxMXHRNalGD2G3/gqa67lclcosG+kPHTfbNf6ov1L6AXi7iwxZvIUJ29QcoP0xvlrvEBQ96lmDuqAoUtesLAGjz1Xlos+u+GOgKCiCrrMSdv88AXHVw9azBkb5fQCpwRoIcDxMXmXVOWw6NCOTr3BA98xK0584b7yCRQurrXbctCJg8ZS0m+1zZj4mqqR3v/xVQu34D/dOfhNely9CXlRn69RUViJmyGwAgbReF43+qESmTspgvORye4yITOlGPu8ZPguuB2m/0ukuXTKtZ9I7Fou8/hFSoa4+UufIci4XkaMsxdO+TaDky3fwOggCpvz/U37phU8dfLBsc0Q3gOS5qElGbx0N6yhWCCLQ9nA1tvaK2Ul9fHHs9GpCKcG9RiQ4u7g08EzW3CJknnu+4EfM/ugcAELBDBt8vrrrAWxShKyhA+ZcqRN4xAdlJn1opUqKmx8Tl5A6q1cjR1pYiClspg+L32g8/QxEmQYA0KhKQSVET7IX9wxfBV8qEZQvGKS9i3PCPAQDtvMfAP7UtdCdPGY2Ofb5MhTK7O37p747b3QrhLWHlDbJ/nCp0cl3fmoqQxbXXD4lajcmUoMTDA7MOpaK3onYpNs+X2CaNqMMxjQb/jhtmcgsVABDkLui0S4v/huy3QnREpjhVSI3S+8AD0P3iDwBo+b9L0GlqDH36/t1QMrvCsC2X6tDVpRzuEo6ybJlckCJKpkP5SiVqvoqG99fGy+ZFTQ3SFsQj8q6eyB7xsZWiJGoaTFxOYFFRa1zSeBm2qzYHIPTD2pJCOgASLy/U9Kq9eWOuSoGj3VfUewYmLXvgLnHBttifETnwX1Bmd4Ow46BRv8ePaQjTxWNOfCxm+u/mtCHZLU4VOjidqMewe8ZA3HukwX3Eft2w4YcVlguKmt2aCk8s79DecKuUqwkyGcalZ+Fhr2vcH4yomXGqkEx0XDIVgQc0gAi4ZRxH/W8nspahCFxdBg9pDUIVprcbIft2u+tFpOzqgAOvxcP9Z+OSXKJWi08m34fZj0mQPeQTK0VIdPOYuBxMlqYck04+irCUCgipfwOASUkmoVcszvX3wg9hi7jYwkH5St3xfugeRA7qjnB9b7it3W3UL928H4FhKgxtMxQ/Rq/h7wHZFc6zOQiNqING1OHr4t6QJeYYkpaBIAASKSCR4sQYd/w9cyk/rJxA9j0fQfXy7tr/+3p8vkoF7q/GOZ3GzCOJbBdHXA5gV7UOLz80DkKNFkJVDQDT21sUPNkHzz37PQCgm2I7AJ6Ydxb/9t+J34+F4/Mn74F0C5fDk/3jiMvOTc+Nw6O/TwUOHIP+72PQnTBNWheT+wJDL2O0VyFGexXy9u9OxlfqjtFehch6VIrSR/sY9YlV1UhaPx2vXmpvpeiIGo+Jy46d05bjj997IXpamsnqMYmXF6S+vpC28MMzT/2I/XGrrBQl2Yrs4R9DOeEcpL6+tVPHAPSVlYh5cg8+23Y7crXlVo6Q6MZwObydytWW44kRE4HMHOgrKkx32BSGd9r+AACIkbuw+C0BAMr11cjQSPDSsMegO3rC0C7x8oKuS1usWfURz32SRXA5vJPQiXpE/ToZLoVSSDQCWmf+bZK0ZG1a4+gLgVjc6ktOCZIJT4krOrtoIMqMv/Dpy8ogu2zmCxCRDWLishOXdBXYWR2A9ktLDTdxNCxzl0ghjWoNCAKKuvkj++7l1gqT7IAEElSFe8E9PxC6/It1HRot1lUGYKDbBQRKPawXINF1cKrQTgw/MQTapMsQ1WqTPlmb1vhwy9fwkcgggYRTPXRdlfoadFo/FTH/2mvULshdUPJLOFK7rrZSZOQsbmWq0Dk+9e1c21WTUfpuuNmkVTBFheqP9AiSusFT4sqkRTfEXeKCOf3W4eyPnSG96kND1NRA8YEfor+cYsXoiK6NU4U2anW5EoeqwgEAkWtrYHL9jUQKfd9YVN1Zjv0dfwHAxRfUOBO88zC093I8dOcMeB3IhfbMWQCA4vc9iKjuCYyxcoBEDeCIy0a9uugx7Ooqx66ucpi7aFTq6YG3vvoQx/p9ZfngyGGEyDyxbelHyPxXmLVDIbphTFw2Zp+6BrdNexKhv+Y0uE/RWBXabFIjRi5YMDIiItvAxGVjCnUe8PjtILRnzxm1y1qFo/hxFYofV+FSghqLW6bxfBY1GaFdOSrujzdcmOxyuQpJx4YjvabKypERmeI5LhuiE/XQmDtXJQi4NCAMaW8us3xQ5BSO9/8Kv/VwxeLfe0BfWQn9waNAArBsz0Asbpl2/ScgsiCOuGxI1LonseSee0xWDxasjcGbL39opajIWdzhWorHDmSgZHSf6+9MZEVMXDagUl+D9tsfR0iKFLr0DEO7LKwlzr3YF9NjNmGgW/27ahE1LXeJC0Z7FULjWXfudOP6Hhh+YogVoyIyxalCG3BZX4O2zxRAm3vY0CZxd0dFl1CkT1tqxcjI2bV+KRUXx6mA160dCVEdJi4blfVpNP7o+y4AT2uHQkRkUzhVaGXTc+OQuGIm9KVlRu2eHtVoK2fSIstT3n8BZ+f0tXYYRA1i4rKiHdV6/LKrJ1rNTa2r8i6RQhoVCW+3ausGR05rc6e1GHn//wzbsioRm6qkUIsaK0ZFVIdThVaiEXV45eFxiNm/D1dXOZa2icBHKV8hSOoGlnEiW+D1fRreXtcHBfuP4mGvImuHQ8QRlzXML+iI+FenQXrynNGdiwumqFC1rLZgLm/8SDZDFKGvqoaOHxdkIzjisrAVpYFYsV+FmGWp0NXrK4rVYX+nteBIi4ioYUxcFvb5zHsR8+tua4dBRGS3mLgszcxtOyUeHhB/9cHiiC8tHw8RkZ3hpLWVSTtEo+DhLvgoahWGuXMlIdmGGNdclIzuA2lQYG2DqMcrB4dhUVFrq8ZFBDBxWZROrFe2SRBwemQA9v5nGSJkvGaLbMcY5SXsems5Knu2qm0QRbR+6BA++maodQMjAqcKLeaDolb4bUx/uJ84Bj0ASKTw2uqLz1p+AC7GICK6cUxcFnJJ6wlxX7rRKa67Aw6ijyuTFhFRY3CqkIiI7AoTFxER2RUmLiIisitMXBbiKa2GNCoSEldXSNzdIY1qDQ9JjbXDIiKyO1ycYSEzfE9i0pYjuOfJp1HtI8Wfb74Lb4mbtcMiIrI7TT7ievnllyEIgtFP+/btDf3V1dVITk5GixYt4OnpiVGjRiE/P7+pw7A5UkECb4kb/GadRsy0o0xaREQ3qVlGXJ06dcJff/1V9yKyupd59tln8dtvv+GHH36At7c3pk2bhpEjR2LHjh3NEYrN+Slqo7VDICKya82SuGQyGYKDg03aS0pK8Omnn+Kbb77BnXfeCQD4/PPP0aFDB+zatQt9+vRpjnCIiMiBNMvijJMnTyI0NBRt2rTB6NGjkZOTAwDYt28fNBoNEhMTDfu2b98eERERSE1NbY5QiIjIwTT5iCs+Ph4rVqxAu3btkJubi/nz5+O2227DkSNHkJeXBxcXF/j4+Bg9JigoCHl5eQ0+p1qthlqtNmyXlpY2ddhERGQnmjxxDRkyxPDvLl26ID4+Hq1atcL3338PN7ebW5CwYMECzJ8/v6lCJCIiO9bs13H5+PggJiYGmZmZCA4ORk1NDYqLi432yc/PN3tO7IrZs2ejpKTE8HP27NlmjpqIiGxVsyeu8vJyZGVlISQkBD179oRcLsemTZsM/RkZGcjJyYFKpWrwORQKBZRKpdEPERE5pyafKnz++ecxYsQItGrVChcuXMC8efMglUrxyCOPwNvbGxMmTMCMGTPg5+cHpVKJp556CiqViisKiYjohjR54jp37hweeeQRFBYWIiAgAP3798euXbsQEBAAAHj33XchkUgwatQoqNVqJCUlYenSpU0dBhEROagmT1zffffdNftdXV2xZMkSLFmypKlfmoiInACL7BIRkV1h4iIiIrvCxEVERHaFiYuIbogQ1xnVoTprh0HE+3ER0Q2QSPH4yj8w2qvQ2pEQccRFRKY+LQlG/6eehPue09YOhcgER1xEZCJbHQCP1WnQAZCFh+Fy/zC0lB2wdlhEAJi4iMgMvSgY/l2QEI7dry+zYjRExpi4iMhI+4+nos33hQAyrB0KkVk8x0VERjwuiNClM2mR7WLiIiIiu8LERUREdoWJi4gAACc0FWi7ajL8/64wtF2Y2Rfi/bx2i2wLF2cQEQDgeE0Aol/YD1FTY2i7/cF9WNwyzYpREZniiIuIiOwKExcREdkVJi4iMiHx8ICo6opgRYm1QyEywXNcRGSiJr49Nn39qbXDIDKLiYuIELVlHMJWyqHQ7rV2KETXxalCIoI0yw2K3/YAogixXzfk9VFYOySiBnHERURGil+sQHr3FdYOg6hBTFxETuycthyPT3oWUYfPQGvtYIhuEBMXkRPTiIDbvtPQFhRA6uON8+M6YXDoTmuHRXRNTFxETkotalCgv+pcVkggUp9fBHeJi/WCIroBTFxETir2fxMQPasIukvnrB0KUaMwcRE5KU2VHNozZwEAZQ/1Qe4gLeSC1MpREV0fExeREzqoVkMor/vzz03QIXvIJwCYuMj2MXERORmNqMOsh/6FmAMHIFo7GKKbwAuQiZzI/IKO6PnfpyDNOg9RUwOJhwdOftEDT/XdZO3QiG4YR1xETmJ1uRJfHFAh+p2d0P3TJrgqsH7gB4iRe1g1NqLGYOIichIfPPMQov/YY+0wiG4ZpwqJnITAE1rkIJi4iBxclqYcj2bfAZditVG7NKYtCoe2gzszGtkZThUSObjlhbehsF8RgCKj9uxHgnDsyaUAPK0SF9HNYuIicmDtP56KyNWXARyva5RIodgcgGVhy60WF9GtYOIickDntOUYsm8SWm5TQ3+oLmlJ20Xh3LBAfBLxHnor5FaMkOjmMXEROaC96mC0fOAERK3xzUoK+gbg0PNLATBpkf3i4gwiIrIrHHEROZj7sxJxeEs0Wut2GbXnzugL2cBCK0VF1HSYuIgchE7U43CNBsd+j0HrBXU3gxTkLpAGBWD449vxetAhK0ZI1DSYuIgcRI62Ev9352iEn9trVDxXMyAWX654H74SV7D6OzkCJi4iB6EHIJZXQtTUGNrOze6L4IRzCJSyFiE5DiYuIgeQXlOFlUV9gXqrCH1uz8Omjr9YKSqi5sFVhUQOYHjKU9jXQwpdUdH1dyaycxxxEdkxjahD3MKn0G5nGUSx7syWrFU4xBU6LGz1I/j9lBwNExeRndqt1uCdC0kIW3sO2tM5hnZJt47IVXkjLWYx5AIXY5DjYeIislPPZjwEz8GnAFw2aj8+zR3ZQ5eBKwjJUXEOgYiI7AoTF5EduvvkYFzeGWzUJigUuPSkCrHR56wUFZFlcKqQyI7oRD3KRTXKXwtDxIarqmPIZJAG+OP7F99CWznvr0WOjYmLyI68cikWe++Lhsv5w0bVMc7P6I3lkxejtczdarERWQoTF5Gd6HdoJEq2BKPlqbqRFgQBZ19UoXXiafRz5cw/OQcmLiIbpxF1SK/RQvd1IFp+XZe0JK6ukAQH4q3xn2GYe7UVIySyLCYuIhv3W6U3PurTGz4lxsVzLz3SHb/MfwuBUndwnRU5EyYuIhvW+8AD0PwWgMDLqYAoGvWJUiBExoUY5Hz4NY3IhpWlBSBw6U6TpCXtEI3KEMFKURFZF0dcRHYo6qvT+DVkFfjdk5wRf+uJbFCuthxxc6cg8sdLRu1CXGfIt4RgWsAWSAX++ZJz4oiLyMasr1Tg/bNDEfh9OnSlpYZ2UdUV5wZ6ID3mawC8MSQ5LyYuIhszZfMYxEzcY9Je/XIp0mO/sEJERLaFiYvIRqhFDQbOfAoddudDd1W7NCoSvVafwGgfjrSIgJs4x7Vt2zaMGDECoaGhEAQBa9asMeoXRRFz585FSEgI3NzckJiYiJMnTxrtc/nyZYwePRpKpRI+Pj6YMGECysvLb+mNENmzXyrcEbd7LPy2nYUuM9vQrr2zJ7IfC8Ec/0OIkTNpEQE3kbgqKirQtWtXLFmyxGz/woUL8f7772P58uVIS0uDh4cHkpKSUF1dd2X/6NGjkZ6ejo0bN2LdunXYtm0bJk2adPPvgsiOVeprMP/4cITedxTac+eN+k49DhybtJQ3hCS6SqOnCocMGYIhQ4aY7RNFEYsWLcKcOXNwzz33AAC+/PJLBAUFYc2aNXj44Ydx7NgxrF+/Hnv27EFcXBwA4IMPPsDQoUPx9ttvIzQ09BbeDpH9uXPW0whOOQ2ttQMhshNNup42OzsbeXl5SExMNLR5e3sjPj4eqampAIDU1FT4+PgYkhYAJCYmQiKRIC0trSnDIbJpGyrliFwzCX57CqDNzTPqk7i748z8vhjW+YiVoiOyXU26OCMvr/aPLygoyKg9KCjI0JeXl4fAwEDjIGQy+Pn5GfapT61WQ61WG7ZLr1oiTGSPsjXlWHz+fsRM3W20EAMAJB4eQGQ4NoxfiAiWdCIyYRdXMC5YsADe3t6Gn/DwcGuHRHRL7n99JjRDy8z2nZnRFZ/9/gmTFlEDmjRxBQfX3ko8Pz/fqD0/P9/QFxwcjIsXLxr1a7VaXL582bBPfbNnz0ZJSYnh5+zZs00ZNpHF7FPXoNPiqQjafhn6igrjTokUJxfH486797F4LtE1NGniioyMRHBwMDZt2mRoKy0tRVpaGlQqFQBApVKhuLgY+/btM+yTkpICvV6P+Ph4s8+rUCigVCqNfojszW61Bv+9kISwN9KgS88w6pP6+gJxHfHxkE+wuCXP9RJdS6PPcZWXlyMzM9OwnZ2djYMHD8LPzw8RERGYPn06Xn31VURHRyMyMhIvvfQSQkNDce+99wIAOnTogMGDB2PixIlYvnw5NBoNpk2bhocffpgrCsmhjf38GUT8x/T2JABw8f72SH15MZe9E92ARieuvXv34o477jBsz5gxAwAwduxYrFixAi+88AIqKiowadIkFBcXo3///li/fj1cXV0Nj1m5ciWmTZuGhIQESCQSjBo1Cu+//34TvB0i23NOW457Xp2JyB2F0JlJWic+icOYXtuYtIhukCCKZv6SbFxpaSm8vb1RdKINlF52sb6EnNSGSjneP5cI3Yhy6MuMF2NIW/ihQhWFRxb+hsk+5xt4BiLHVFqmh2/MKZSUlDT69A9rFRI1oyf/Go+YybvN9lXGt8XWjz6ycERE9o+Ji6gZVOprkDDzaXTYk29ynRZQOz04p9+vFo+LyBFwno2oif1S4Y4+e8fCb8tpo4K5ACD18UbBFBVGx6Vhgrf5C+6J6No44iJqQpX6Gsw7+jBC7j1mUntQkMmgjwpH6pz3oRDkVomPyBEwcRE1oTtnPY3QBgrmnvhvHFaMWM6kRXSLOFVI1AQ2VMoR+csk+O2+aLZgbs7LfTG07wEMcG3gCYjohnHERXSLcrT/FMydbKZgrrs7hMhw/PkEC+YSNRWOuIhu0X2vzYRmeIXZvtMzu+GjPz5l0iJqQkxcRDdpt1qDjsv+KZhb7+JiSKQ4+X48EobvQxiTFlGT4lQh0U3YrdbgnQtJCH8tDTq98QSh1NcX+qgwrBj+Ic9pETUDjriIbsK4T55B0W0lgN708uL8B9rj1zUrmLSImglHXESNkKMtx32vzUTr7YUmIy2gtiLG6LjtLJhL1IyYuIhu0IZKORafvx+B3xyBzkzB3Mo+bTG732+Y5H3BShESOQcmLqIbdM2CuX3aYsvHH1s4IiLnxMRFdB2V+hrcOetpdNh9scGCubP7/WbxuIicFRdnEF3DLxXu6LdvDFqknIbu5CmjPqmvLy5Nqi2Yy+lBIsvhiIuoAWpRgznp95gtmAuJFPqoMOyat5gLMYgsjImLqAEDn38K4VvMF8w9+V4cVgz7kEmLyAo4VUhUz5WCub578s0XzJ3XF8NV+3mdFpGVcMRFdJVz2nK8f+6BaxbM3ThhIcs4EVkRR1xEV7nn1ZnQjSg323f6hW5Y/senTFpEVsbERYTa2oMdlk9F0A7TgrmCTIaT7/VBwrB9rPJOZAM4VUhOb5+6BosuJCHiP6nQiaKhXRrdBqKbC0QXGVaMWM5zWkQ2gomLnN6Yj6cj7I004KqkBQA9fjiJlwL2AwAUgtwaoRGRGUxc5JT6HRqJmlVBAIBWu+sVzO0dC/WrZRjjuxIKwcNKERJRQ5i4yKnoRD1eutgNJVuC0fLznbVtV/ULvWJxNsEL6Z2/AsCkRWSLmLjIqZSLahx8MAotT+402y++UYT0Dl9ZOCoiagwmLnIad58cjIr5LSHPOWLSJ20XhU7fZmFii1XgSIvItjFxkVN48FQCTm5qg4iUnRDr9enu6IGz/V3xa9D3kPKcFpHNY+Iih6YT9VCLWhTNjkDE/0ynBwW5C85M1OPkwKXgZY1E9oGJixzaC3lxOPZ4FKRZR01GWtKgQNy9OR1DPVMA8MJiInvBxEUOa8Dh+1C4NQRhR01HWppBcci6S45HlGvhLWHSIrInTFzkcDSiDtnaaug+CUTYD6ZJS9rCD5lDZch6cBkAN8sHSES3hImLHM63ZUH47rbu8CraZzI9KCgUuHf7cTzguQ6AuzXCI6JbxMRFDqX3gQeg3hCA4ALTkZb2zp7IHitiqMcm+Eo5PUhkr5i4yCFoRB1+q/SG5rcABC81Mz0YFYmcAQqcumspuBCDyL4xcZFDSK/R4qM+vRF4OdVsf6/VJ7DW/0cAUssGRkRNjomL7F6/QyOh+zoQPiV7TSq8i6quqH65FKN9voacFxcTOQQmLrJbOlGPVy7F1hbM/dq0IoYQ1xnnBnogPfYLsIwTkeNg4iK7VS6qsfe+aLQ8Zb5gruztQqTHfG3hqIiouTFxkV0afmIIKl9vCZfzh036pB2iEfN1Nqb6/wiOtIgcDxMX2Z0HTyXg1KZIhG8wUzB3YA+cvc0VvwavYsFcIgfFxEV2pVJfg6J/hyN8u5mCuTIZcp7U4cTtLJhL5MiYuMhuTM+Nw4kxbSHNPGYy0pIFB2FoylGM8NwCXqdF5NiYuMguDDxyLy5uDUV4uvmCuacS5VilXAtPFswlcnhMXGTTNKIOOdoqaD8KQviPZipi+Pri5DAZTj2wDICr5QMkIotj4iKb9mVpS6we2AWelw+YTA9KXF0xaucxjPT8DSyYS+Q8mLjIZvU+8ADUGwMQnG+mYG5CT5x6DEjy+IsFc4mcDBMX2RyNqMOGKg9of/VH8HLTpCVr0xpnBiiQncSCuUTOiImLbM6hGh2W9FIhoHiX2f7eP5/AGv/VYMFcIufExEU2oVxfDdWiGVAUi5CqAZ+S3UYFc7PeUiEoNh8A8IjPVyyYS+TEmLjI6vapa/BRwR2I+PY0tOcvGPVJPDyg7tcBw+/cg0Uhe/9pZdIicmZMXGR1Y/aPR9iodABVJn1ixzbY+PlHkAqshEFEtZi4yKq6vz4VEf8rgt5M36k3VBg/NIVJi4iM8BOBrGK3WoOBR+5FyKYC6P8+ZtQncXVF8RgVuvc/gRf9M6wUIRHZKo64yOI0og4Lzw2BYtBp6Op3CgIkwYH49bW3ESjluSwiMsURF1lc3FtPoWqs+euvzv6fChM3pqCFxM3CURGRvWDiIos5VFONyD8nIGR7KbSnThv1CTIZLszsi9YJp3GvRznPaxFRgzhVSBZxSVeBlUX9EPPEfoiicdVBQaGANMAfH0/5AH1ceVExEV0bv9aSRQz4cCYOJ/oZXVR8Rf6Ennh3x/fopRCsEBkR2RsmLmpWOdpyxHw5BWEpFdAVXjbpP/2qCuEPnUKM3IPTg0R0QzhVSM3mhKYC3xb3RttX/oa+stKoT+LuDqFVSzw3ai0meV9o4BmIiEw1+ivutm3bMGLECISGhkIQBKxZs8aof9y4cRAEwehn8ODBRvtcvnwZo0ePhlKphI+PDyZMmIDy8vJbeiNkewaveQ6pcZ4mSQsAyobE4seNK5m0iKjRGp24Kioq0LVrVyxZsqTBfQYPHozc3FzDz7fffmvUP3r0aKSnp2Pjxo1Yt24dtm3bhkmTJjU+erJJ5fpqxC6aiqjvqiBqakz6T72pQvTMo3CXuFghOiKyd42eKhwyZAiGDBlyzX0UCgWCg4PN9h07dgzr16/Hnj17EBcXBwD44IMPMHToULz99tsIDQ1tbEhkQw6q1fjk0u2I+DIL2rx8oz6Jhwdq4ttjWOLVBXOJiBqnWc6Gb9myBYGBgWjXrh2mTJmCwsJCQ19qaip8fHwMSQsAEhMTIZFIkJaWZvb51Go1SktLjX7INj26bwJO9lKbJC0A0Me2xYavPmbSIqJb0uSLMwYPHoyRI0ciMjISWVlZePHFFzFkyBCkpqZCKpUiLy8PgYGBxkHIZPDz80NeXp7Z51ywYAHmz5/f1KFSE5mV3w1p/9cLANDqQoVRwVxZcBBa/FwFpUyNYMVurhwkolvW5Inr4YcfNvw7NjYWXbp0Qdu2bbFlyxYkJCTc1HPOnj0bM2bMMGyXlpYiPDz8lmOlWzftfDz+2NEdUb/X3q346qQlxHXGuX5KrAp/B54SV+sESEQOp9mXw7dp0wb+/v7IzMxEQkICgoODcfHiRaN9tFotLl++3OB5MYVCAYVC0dyhUiNpRB0OvtENUat3mXZKpDj5mCeyHlwKgEmLiJpOs8/bnDt3DoWFhQgJCQEAqFQqFBcXY9++fYZ9UlJSoNfrER8f39zhUBP5tCQYdw99DF4bjpr0SVxd0WWvHr/c+64VIiMiR9foEVd5eTkyMzMN29nZ2Th48CD8/Pzg5+eH+fPnY9SoUQgODkZWVhZeeOEFREVFISkpCQDQoUMHDB48GBMnTsTy5cuh0Wgwbdo0PPzww1xRaEdKdO4m99ECAKFXLE4P88LSFgsRKTdfAZ6I6FY0esS1d+9edO/eHd27dwcAzJgxA927d8fcuXMhlUpx6NAh3H333YiJicGECRPQs2dP/O9//zOa6lu5ciXat2+PhIQEDB06FP3798dHH33UdO+KmlWRrhK5Nd4m7VKlEmcTvXBs0lImLSJqNoJYv1S3HSgtLYW3tzeKTrSB0our1Cyt45KpaP1hBnSXCo3a2+2VY3bgZoTImLSI6NpKy/TwjTmFkpISKJXKRj2Wn/p0w3K05Yj+cgrCNlcYJS2heyec+LAXnmixnUmLiJodi+zSDcnSlOPr4t6IqlcwVxYehgt9vZE9YikArvwkoubHxEU3ZNBPzyP6hf0QNcYFc+Vfa7Ar6j0AcusERkROh1OFdE0l+ip0fm8q2n5vXDBX2i4Kl36NwczwP6AQmLSIyHI44qIGHVSr8VnhALT6wrRgrraFB/b1/AL87kNElsZPHWrQQ7snIiNOY7ZgLhGRtXDERWZ1WzAVkdtLYO5aiVNvqPDokG0Wj4mICOCIi+rZp65B0rHhCP0zH+KBdKM+iasrSh/tg+79T2B+QHoDz0BE1Lw44iIDnajH/Jy7gYRz0NXvFARIQoPx+5vvwFfqbo3wiIgAcMRFV+n55jRoJ7iZ7cuZq8LY9Zuh5O1JiMjKmLgIh2qq0TZlPIJ3lECXmW3UJ8hkyJ3RF5EDT+NBzxLeCJKIrI5ThU6uSFeJLwr7I+qxAyYLMQS5C6RBAfj8qUXoqXCxSnxERPXx67OT67/0eRxL8jXblzc5Dgu3/4huLvx+Q0S2g4nLSWVryhH99RS03FphUuUdAM68okLYqGx0cnHj9CAR2RR+lXZSZ7RKRM09AH11tVG7xN0dQlgIXnzwB4xRXrJSdEREDWPiIiOlw2Lx57vvwZOrB4nIRjFxOaE+B++H7ocA+NXsNmo/tVAF1W3pTFpEZNOYuJyITtTjnaJoVGwOROjnOw3tEg8PaHq1w8i7UvFm0EHrBUhEdAOYuJxIob4KWwa3R+i5nUbtum7R+Oubz6wUFRFR4zBxOYmkY8OhficErgWHjNpPvanCQ0nbrRQVEVHjcZ2zExifcxvObomA4rc9ENVqoz7XmBK8GnjYSpERETUeR1wOTifqkTutFcL37jTtFATLB0REdIs44nJSsvAwJB0uwZoeH1k7FCKiRmHicmC/Vbqi3dYnIL1UatSuTeiJk1PCMdnnONrKPa0UHRHRzeFUoYO6pKvAm1n3oe2jB6G9ql2qVOLE/VJk37MMAAvnEpH9YeJyUMP+73m0+C3D6IaQEi8vjEo7gRGefwDwsFZoRES3hFOFDmZbNRD95RS02HMJusLLhnZNYk8cf6sDRnhmIVDKpEVE9osjLgdyrKYSiy+MQJt/pxqNtGStwnH6Dhdk370MHGkRkb3jiMuBPPze8yhNKDduFAQM/O0oDo173zpBERE1MSYuByJoYXKBMQB4S6ugEORWiIiIqOkxcTmASn0N3rrcFq5FeqN2aQs/6Ad0QwtpeQOPJCKyPzzH5QBS1W5IiQuAd/Uuo/aipBikvr3cSlERETUPJi47F7NtDEK/UECh3mvUfuKzOEztvdFKURERNR8mLjsnnvaA4o9Uw7a0hR+KkmIwtfdGzPTLsmJkRETNg4nLjulEPeqXydV0iOD0IBE5NCYuO3VRV4GH/vUMog9lG5V0IiJydExcdmh5cUss3JOE9mknoS0uMbRX3hePC7fzViVE5NiYuOzQ2wfvQvTY/SZ1CCufKEJWz++tFhcRkSUwcTkAWWQrzPhrHXq4lAFwt3Y4RETNionLzkSu/xeCNhn/t4lSCeIU5fCWMGkRkeNj4rITRbpK7K/xQvtF5dAfOm5ol7bwg6alD6Qm6wuJiBwTE5edGHtqJDRJxdCrM4zaj8+NwYFR78JT4malyIiILIuJy05o9RLoq6sN2xJXV5z4pAOmddsAbyYtInIiTFx24LsyX2ScC0IULhjaBBcXrO63HN0UCitGRkRkeUxcdmDpCw8gau1ua4dBRGQTeFsTG/ZlqT/6P/UkPHedNmovflyFoA0iYuRckEFEzocjLhv11uW2WH5gAKJWpxldaKwe1gsX76xBWsT/ALhYKzwiIqth4rJR3yxNQtTSncaNgoDH3/4VE7zzrBMUEZEN4FQhERHZFSYuG5OrLUeHHY/DN7PGqF3aLgoXZqrQXnGhgUcSETkHThXakEp9DVKqWqHVYycgqtWGdomrKwr6BeDw9KXgdw0icnZMXDak809PocObZyGqjUdVNesCsTL6bQAe1gmMiMiG8Ou7DSjXVyNyzSRE/KmH9nxd0pJFtkLmoj54ptVfiJEzaRERARxxWd1FXQW2VIWiw/zT0OVfNLRLAwJQHBeMrAeXWzE6IiLbw8RlZf3+l4yof52AvvKiUXvOh4HY0XsRANYhJCK6GhOXlehEPTp8mYywrVroKytN+l1kWhbPJSIyg4nLSvQQ0XZVCfQHjxq1C3IXiD3bI9jrkpUiIyKybUxcNkbSOgzf/vghR1tERA3gqkIreC63B+54ZiqQddao/cLzfRG+MheeAm9VQkTUEI64LOzF/C74eUcvRP+YBv0/bYJMhooRPeE68BI+DEsFv08QETWMicvCdrzUB9Hr0ozaJF5eWPLue+ji4mqlqIiI7EejvtovWLAAvXr1gpeXFwIDA3HvvfciIyPDaJ/q6mokJyejRYsW8PT0xKhRo5Cfn2+0T05ODoYNGwZ3d3cEBgZi5syZ0Gq1t/5ubNiS4nAMGjUWHtuOG7UXTFFh2I5MtJNLrRQZEZF9aVTi2rp1K5KTk7Fr1y5s3LgRGo0GgwYNQkVFhWGfZ599Fr/++it++OEHbN26FRcuXMDIkSMN/TqdDsOGDUNNTQ127tyJL774AitWrMDcuXOb7l3ZoIsaJYTUv6ErLTVqV/sKSPY5C4Ugt1JkRET2RRBFUbzZBxcUFCAwMBBbt27FgAEDUFJSgoCAAHzzzTe4//77AQDHjx9Hhw4dkJqaij59+uCPP/7A8OHDceHCBQQFBQEAli9fjlmzZqGgoAAuLte/OWJpaSm8vb1RdKINlF62fz6oXF+NOfn9cayn8ahS4uqK0y/0wLHJS60UGRGRdZSW6eEbcwolJSVQKpWNeuwtfeqXlJQAAPz8/AAA+/btg0ajQWJiomGf9u3bIyIiAqmpqQCA1NRUxMbGGpIWACQlJaG0tBTp6elmX0etVqO0tNTox570WTwDJ4f7GzdKpGi3Q4vfJyy0TlBERHbqphOXXq/H9OnT0a9fP3Tu3BkAkJeXBxcXF/j4+BjtGxQUhLy8PMM+VyetK/1X+sxZsGABvL29DT/h4eE3G7ZFZWnK0eaHyQjdWgFtbt17E3p2QuY7vfBEi+2IlHtaMUIiIvtz04krOTkZR44cwXfffdeU8Zg1e/ZslJSUGH7Onj17/QfZgOMaf8S8cABC6t9G7UUdlch6cDlXERIR3YSbWg4/bdo0rFu3Dtu2bUNYWJihPTg4GDU1NSguLjYadeXn5yM4ONiwz+7du42e78qqwyv71KdQKKBQ8KJcIiJq5IhLFEVMmzYNP//8M1JSUhAZGWnU37NnT8jlcmzatMnQlpGRgZycHKhUKgCASqXC4cOHcfFiXTX0jRs3QqlUomPHjrfyXmxKn4P346W3x0OsqTFqz35DhfBJJ60UFRGR/WvUiCs5ORnffPMN1q5dCy8vL8M5KW9vb7i5ucHb2xsTJkzAjBkz4OfnB6VSiaeeegoqlQp9+vQBAAwaNAgdO3bE448/joULFyIvLw9z5sxBcnKyQ42qig4EoPXyVMO2xMMD+ti2eGTINswPML8IhYiIrq9RiWvZsmUAgIEDBxq1f/755xg3bhwA4N1334VEIsGoUaOgVquRlJSEpUvrlntLpVKsW7cOU6ZMgUqlgoeHB8aOHYtXXnnl1t6JjdP0boc/v/4EUsH2l+8TEdmyW7qOy1ps+TquS7oKDJ7/PAJTL0OXXldVRHdHD/y18jMrRkZEZDtu5Tou1ipsQhsq5Xgn50EErj4OXVGRoV1UdUVePFcQEhE1BSauJjR55+OIHrMfunrtRf9XhSM9vrBKTEREjsa25tmIiIiug4mrCehEPfofGgnlXuPpQKlSifyn+2JASKaVIiMicjycKmwCeojwmqWAx987DW2C3AViZEvsfGER3CXXLxxMREQ3homrmWQu6IFvRn3ApEVE1MQ4VXiLPi0JRvsfkiG5WGTUrvPUo7eC99giImpqHHHdghOaCiw6fieipu/C1XfakrUMheDu2Hd0JiKyFiauWzDh2RkI+/1v6K9qk/p4Y8a2P6FSVAHgNCERUVPjVOEtkFbroa+uNm4UJAiWlvPcFhFRM+GI6yZc0lXgx7IYyCuMpwNlwUGo7hgGV6H+JchERNRUmLhuwqLCPtjTXQaJeMCoPXtCWxyeuhhSgXc1JiJqLkxcN+vq2sQSKXJWdcS0Tr+y+jsRUTPjp2wjLSpqje+O9jRpn9PldyT7nLVCREREzoWJq5G+en8I2j560KhNkAjWCYaIyAlxqvAGbasGXpnwBIKOZBpVf6+8Lx7DX0nBcI9zANyu+zxd35qKwD2VzRKj9JUCrG//W7M8NxGRrWDiukHFOnfIth+BTlNj1K72lmBWi5O4krQq9TUY+PdoVGvMH9rQbSUQ96U3S4xnN/ZFl9JHzPbdFZ6B/4bsb5bXJSKyJCauG6AWNSjWuZu0CwoFRAlQoq8ytJ3RCvB/ogS6/Itmn6s5bzcdtmBng30bn+uLkmd3mLS7Cy6QC9JmjIqIqGkJoig252dpsygtLYW3tzeKTrSB0qv5T9O1/X4y2r97Htozxosv9JvCcbnSDcHPXDUK0+tr97Oxwyrx8oIkoIVJ++UlUqR2XW2FiIjImZWW6eEbcwolJSVQKpWNeixHXNdQoq9Ct9+fRquNOpOkBQC5G8IhrwC0pxoe6dSX/3RfVIQ1X1JzLRAQ+pZpPPqyMujLykzaNT+p0CbzScO2T6ti7I9b1WzxERHdKiauBlzSVWBrVQg6vHgKukuFZvdp+ab5hCULCQYk5keCA8fuxqKQvU0WZ32Lilpj47edTEZ8YkUFdMUlJvu3+DgVV4/DKu+Lx8FYtdE+sS5yXp9GRDaDU4UNaLtpPGKmZpodpVyL1Mcb0/fuRKxLkdl+f6lbs55T0ol6XNSZrlrst+VpRI+5gcUZEimknh6GTcHLE69sX4OeCtZeJKKmw6nCJqQT9Wi/MhnhW7TXTVrZr6ugb11l1CaT6RDvuh7eEuuUfZIKEoTITF/7hbg/8fY3d5m0h38ph8v6PXUNeh10paWGTaGyEk8smg6dG6DxFHFg3HssIExEVsURVz1qUYN7hzwO/aHjDe4jcXeH2CESqs8OYF7A0SZ9fUuLWTEFbVbXJSpJaRV0J0+Z3VfWMhQD/8yAn7TC0DbUI9NsoiSixtlQKcdZjfECKqmgxyNe56EQHO+mtBxxWVhNnw7Y8NXHDnHe59jYJcDYuu1RmUNQdbv5fbXnL+CvWCWAul+yjds74rvIlOYNksgJzJs7Acpv04zaJO7uaHP4Iga4WikoG8XEdZWnL/TCnv/2hE92wxcIn3pDhaSE/Q6RtACYvI+XIn7FvC33GLYrNC5we7QSuoKC2oZ6A/SL8yKhCpwMAJg9/0vc7dE8VUGIHFHk+n8h+K/aj2G/bWehrff3pa+qxoszn4ROcf2ycs/MW4WHvcyfW3c0nCq8ysAj90Ix6LRJu6RLe1S0qR1ltJ51HF+22tZkr2nrSvRVuGvODLgW1xW68vo7H9rsMyb7nv2xM+LDzkAm6PFuy03wlPBrItEVOlGPZ3PjUaat+7s4trgTvL/e1STPf3pVF/RtlQ0JRLwTthHekuuXoLMmThU2s+PTvJA9/CNrh2EV3hI37H59mVFbl7enIuQd08QVfv8RXEDt9MaRI3L0Yd4iMijVVyPrgVCjL33eaJqkBQCtHzqEC6it6JOWrsQgd02TPbet4YjrH13fnIqWfxVCl55haJP6eCN8Qw0mBGxDb4XjnRy9Wb9VuuJAZWsAQMrz/SHfUO+6NEGA2KcLRBcJqvxd8Nt7i2z+2x9RU+r0wVSE7jRecQydCGnaUYj16p1ekflVd4zvmmrUVqZzxZERLaE9d/7GX1wQgPhYnJwsx6lBnzY2dIvhiOsWHFSrMf7QWIRuLTZKWkJcZ5wdqMSnwQu5aq6eYe7VGOZeu+rys3tuh3d0X0OfT5YGLuv3QEj9GwIApX8LqHZNhEKuRZBXGavXk8Mp0lVi0N/joNXVfYluua0Swo6DJvvWHyXUJMWhOKr28pJJ3Tb+U7C7TqW+BnFjp0NR3Oq6cYSuzoI2Lx8QJLjc0QP+AZca/V7shVMnrkp9DT66dDsC7zkOfb2+M0OVODZ5KQAmrWs5dd+HwH112513jUbYZgVEdW31Dd2lQoTfX1t5ROwdi6LVlZALEkgg4fVgZNfK9dUAgD1qb/g/dB76iorrPKJ2Gu9qef9S41i/Txrc313igqPJS28onoTjEyAvKobEzRVv/N9HSHDTXf9BdsqpE1f8+9MR8W0OgHPWDsVhbIz7EFsPheOrhL4m0xvCgQyMvmM0IAjISwjC/rnLGngWIttWpKvEg48mQ55XAkGrg77C9JxvfRJXV/TbXYze7lmGtliXTWiqL8evffwRSvWukAp6DHCtAeC4d31wusT1aPYdSP07GgAQvb0C2rP1kpYg4Ny/VQi73bSoLl1fiMwTd3vk48VZYZBVREBWISDijb0QNTUQNTXQZWYDAILcFYhcOwk/Dl7MclJk81aXK/H8Xw8btgWtgPaHj5ut/wkABZNVKG9t3KaXAh/6vIUIo1MPTTej08dVCuDKggzHTVqAEy7OaP/xVLSaZ744rqBQQBoYgIkpW3CvR3lThOr09qlr8NJdD0GoUkOs0dRdD/aP1rvdMCVgCySCiE5yF4e5Po7s2zltOS7p6hZkTct4BB6DzVeUgSBAFhxkVFjb94dKfN16SzNHad+4OKOJFD3UA2tefQuBUncA/ABtCj0VLvg05SsAwJLLKuzpLjO6iDknQYYXhbsgeLjjxR2/ox+X0JMNGLz0BUQsPWLY9tLlm5wHv0IW1hKvbVuNIGnd8nM/qQIAVyI3Fyauq+il4ArCZnDlmD7isxvffT0VACA/4YaI+TsNhYyFykpMfW8atO6A1kPEvnHv8gJmsohyfTV6fvEsZOV11SnC/yo1KjZ9NfWQXjj3mNawrXCtQQcXCRQCPzssxWkSl0bU4fvyQLiYn5KGtF0UKkOuX1aFbl4nFzdk3vE5AODpdr1w4pdOAABpURm0p3MQ/F7tFK4sOAjL7u2EAFkZfKSVnLalJpWrLceflW0M2wVaL0QtzoY2N8/QVv/8iVSphL5tOADgwu0yZN7xcb09OLqyJKc5x5WlKcfT8aNqr3MwI3qPAu+FpvIciwVpxNrlul1TxyL8/iPGnZJ/Ti737oTfV6/g/ws1mfuzElF2e72afvprLx0vfyAeWxbVLkuXQODvYxPgOa7rSDh6N6o+CYWysOEbKcokOv4yWtiVG2q+0+0HvLup9l5h+WsiEPTBTsMHiTTjLPrNmgpREHC5E3ByDJfQU+OtLldi4X9GAwDcL2rgor/2xbk58/oiYkCOYbuPz55mvQEsNY7DJ65Z+d2QuzUM4d/tNBn+A4DU1xcV/aLR2nWDxWOjWoPd1RjcYR0AoFPxaFRn9wYAeB7Jg/Z0jqEIqdfAHnjyThUAoLdXNiZ455l/QnJ6ayo88UdRrGF7+9k2CPsq1fzOEilq7uoOvbzui2vbO7KxLuaP5g6TbpJDTxWqRQ3uG/SYUSmn+jSD4pCyouEr18l6Yt+ditC3zF+6kP90Xxz8941VFCDnE7VyCtrObCBR1SPx8sL8QymsR2phnCq8SSc+7IU5A361dhjUgNcmrcDChMHwGJJtch+wlj+cQuKhJwAAp54ATiV+Zo0QyYYMOHwfXBb4AgDanbkAbQP71QzuhYFv7jBsy4WL6Cy3u+/vTs2pE1doq0JON9mwuz0q4RG9Fs9PfRIQAY98HTxW194hVpubB+k/q8BaRKrQP3AkNsWucshbnFPDtlRJMCP9AQCA5n8tELqldoR+ddKqGdwLxW3qfi+KO2sxL+BovWdi9RZ74rSJS1AoIBX4LcvWJbjpcOD/aqcEx+fchtx1/xQp1ekgams/nvw+T4VsQyjO7KxBqLRudZibwEocjkYtagyrUQHgtdMPIOBuM6cCJFII8tqPt8uTy/F3728tFSJZgFMmLml0G0z6fQP6uqYA8LB2OHSD3mn5J9LSa6eCpmx7HDFP1N0HTJubj+kJjxmV3fFbUciyOw6m24fPIPK7uktaXCqrzU4Jnn6lN957uHb6uGsTFrIl2+B0iatyZDzOJYoY5l4CucCkZU98pe4Y7F57u5QxPVPx7esDAAAhqTq4/rrbUMD3ikOr+yIyOgYA8O2gZf8UISV7Ua6vRpe/kiFW1/2/td1WDd2JLLP762/rjjNDaqut9Lg9w/C7wqTleBw+cWn83CHz8oK+rAzSoEBcuK8G2Qmfw9GrJzu6+QHpmD8uHQAQGTARHfcEAQD0pWXQV1YCAEL+uxMh/+z/8a7b4RW8ERKIiJG7cgrRxmRrylEpGv9Nntcq0eHf5xosGiBt4QdBXnfu6tSdbjgxjitNnYFDL4cHasu79P1tBjrMycRzu7ciXlHBGngOplxfjcv62gmj4e+9gJD/mi6hl3h5QZDJILgqMHP7Bgx0a6hkKllD3xmT4fNnvXNVor7B24YAgGJrMBZH/mjY9hIk8JW6N1eI1MS4HP4aQmSemNhvK1Ys7oM4RTk8JW7WDomamKfEFZ7/fH/pdv8RbO/aAwAQvVQL7DoEAIZivpBI8cziydD8U8x3z5h34M3fCYs6oanAPZ/NhHDVyanWe3OhKypq+EH/yHumL8p61t55+OOwL+rd24qchcOPuMh5xb4zFWF/FRu2JcXl0J6uK+MjDQrEHZuyECQrgVJazWK+zeSEpgK7qloZtneVReFUfxGiWt3gY6T+LSCGBZm0V79Vgc2d1jZLnGRZtzLiYuIih6UT9dBfVeir557HEHLvMeOd/inmK/TogF/XfsF6dM2gbcp4RI05ZNx4naK252f1xYGnPzBp5/+P4+BUIZEZUkFitATn7c4/4tU/h8NrXFXdLSz++QCVZJ5F/39PAwSgqANwYiyL+TbWyrIWeP+1B03a22ZVXTdRnVjaG1Htcg3bjwduZJKiBnHERU6lUl+D2196Bh4XtRC0Ilw2HjD5UNX374bod48DAHp6nmZ1lXo0og7/zuuFKr1xtYmU7Gi0evDwdR8vcXVF1Z2xwFW3vxu0YBte9G+4pig5Hk4VEt2Eg2o1Xuw+6Jor1y5O7YsDc7jE+mo52nJM6d3wve2uRxrdBt9v/oare50cpwqJbkIHFwnitxZAI0qxLT8KboNPmxTzDf35FBKOTzB5bOXMYqR2XW2hSC1vSXE4fpo2yGyfoBchu3TIbJ/xjgLUf7ZC/0DjC4a9pIfhJrA2IN08Ji5yWgpBbii2uk15GNOfnAJBBNwL9HD/qa6YryzXdKqwvEdf9NY/YNL+eacv0cnFPpbXrygNxNKs2832FZzxRUzK7gYfa26aRhYchLx72tTtIwCLopZhgNmBFWdK6OZxqpConqnn++BU338uUBb1hmK+N6LzPgn+E7TLpN1dYt0RRqW+xqSt0/qpiPnXXjN7X58gN30/NXd04b3t6IbxHBdREyrRV2F7dW0x36d2PIrosftv+LGy1hEQFcYf6lo/D3y2agnCrHSx7KuX2mPnA51NO4pLocu/2OjnkyqVSNx5Du1cLxi1+0gq0c+Vf490Y3iOi6gJeUvcMMy9tjrDge478dVrd5jsE7GhGpKtB0zar77A+QqplxcGrHkOokvD3xHbROVhU8dfGhXnpyXBeHXr3dfdz+28DOEZ5u8k3RBZ6whkTmxptk8vB1Z4/4FAaf0i1UxaZBlMXETXMMf/OOaMP27S3k4/BW2PB5q06wsvm0wt6svKEP102jVfp2icCsdermxUbO8eS0DM5IbPQ12PxN0dgpf5UWBpjxBkjL/WtWy8swJZD6cKiW5Cka4SZaJxoV6dCEwc+zSkW258avEKQe4CiWfjkoFYUwN9RUWjX+uKc7P74o8pC832yVFb55OouVhsqnDBggX46aefcPz4cbi5uaFv375488030a5dO8M+AwcOxNatW40e9+STT2L58uWG7ZycHEyZMgWbN2+Gp6cnxo4diwULFkAm4wCQ7IOv1B2+ZtpLni9H4bieRm1itRTtZxwx3G7FHFFTA12R6QKKpiDtGIPjL3iZtCd0+JtFaskuNSpTbN26FcnJyejVqxe0Wi1efPFFDBo0CEePHoWHR923xYkTJ+KVV14xbLu7191qQKfTYdiwYQgODsbOnTuRm5uLMWPGQC6X4/XXX2+Ct0RkPXt6fG/SlqUpx5SvpkFaVn3tB+sBMeMURM2NJTCpjzfEiJDr7lcQ54tTg1jCihzHLU0VFhQUIDAwEFu3bsWAAbV3ox04cCC6deuGRYsWmX3MH3/8geHDh+PChQsICqqt/rx8+XLMmjULBQUFcHG5/rJhThWSvdGIOuhx7XuAVeo1GD1oLHTHTt7QcxZOUGHHK+9fdz8JJKz7RzbHaqsKS0pqS+X4+fkZta9cuRJff/01goODMWLECLz00kuGUVdqaipiY2MNSQsAkpKSMGXKFKSnp6N79+4mr6NWq6G+6hYIpaWltxI2kcXVJo5rJw+ZRArXD4txubr1DT3nHf67oBDk19+RyMHcdOLS6/WYPn06+vXrh86d664RefTRR9GqVSuEhobi0KFDmDVrFjIyMvDTTz8BAPLy8oySFgDDdl6e+WKmCxYswPz58282VCK7IBUk+Clqo7XDILJ5N524kpOTceTIEWzfvt2ofdKkSYZ/x8bGIiQkBAkJCcjKykLbtm1v6rVmz56NGTNmGLZLS0sRHh5+c4ETEZFdu6kTRNOmTcO6deuwefNmhIWFXXPf+Ph4AEBmZiYAIDg4GPn5xlWlr2wHBwebfQ6FQgGlUmn0Q0REzqlRiUsURUybNg0///wzUlJSEBkZed3HHDx4EAAQElK7+kmlUuHw4cO4eLGu1MzGjRuhVCrRsWPHxoRDREROqFFThcnJyfjmm2+wdu1aeHl5Gc5JeXt7w83NDVlZWfjmm28wdOhQtGjRAocOHcKzzz6LAQMGoEuXLgCAQYMGoWPHjnj88cexcOFC5OXlYc6cOUhOToZCoWj6d0hERA6lUcvhBUEw2/75559j3LhxOHv2LB577DEcOXIEFRUVCA8Px3333Yc5c+YYTe+dOXMGU6ZMwZYtW+Dh4YGxY8fijTfeuOELkLkcnojIvrE6PBER2RWnqw5/JdeWll/7gk4iIrJNVz6/b2bsZJeJq6ysDADQqsdp6wZCRES3pKysDN7e3o16jF1OFer1emRkZKBjx444e/Ysl8ebceVaNx4f83h8ro3H5/p4jK7tesdHFEWUlZUhNDQUEknjTvnY5YhLIpGgZcvam9zxuq5r4/G5Nh6fa+PxuT4eo2u71vFp7EjrCq5sICIiu8LERUREdsVuE5dCocC8efN40XIDeHyujcfn2nh8ro/H6Nqa8/jY5eIMIiJyXnY74iIiIufExEVERHaFiYuIiOwKExcREdkVu0xcS5YsQevWreHq6or4+Hjs3r3b2iFZxcsvvwxBEIx+2rdvb+ivrq5GcnIyWrRoAU9PT4waNcrkJp6OZtu2bRgxYgRCQ0MhCALWrFlj1C+KIubOnYuQkBC4ubkhMTERJ0+eNNrn8uXLGD16NJRKJXx8fDBhwgSUl5db8F00n+sdn3Hjxpn8Tg0ePNhoH0c9PgsWLECvXr3g5eWFwMBA3HvvvcjIyDDa50b+pnJycjBs2DC4u7sjMDAQM2fOhFarteRbaTY3cowGDhxo8js0efJko31u9RjZXeJatWoVZsyYgXnz5mH//v3o2rUrkpKSjG5M6Uw6deqE3Nxcw8/27dsNfc8++yx+/fVX/PDDD9i6dSsuXLiAkSNHWjHa5ldRUYGuXbtiyZIlZvsXLlyI999/H8uXL0daWho8PDyQlJSE6upqwz6jR49Geno6Nm7ciHXr1mHbtm2YNGmSpd5Cs7re8QGAwYMHG/1Offvtt0b9jnp8tm7diuTkZOzatQsbN26ERqPBoEGDUFFRYdjnen9TOp0Ow4YNQ01NDXbu3IkvvvgCK1aswNy5c63xlprcjRwjAJg4caLR79DChQsNfU1yjEQ707t3bzE5OdmwrdPpxNDQUHHBggVWjMo65s2bJ3bt2tVsX3FxsSiXy8UffvjB0Hbs2DERgJiammqhCK0LgPjzzz8btvV6vRgcHCy+9dZbhrbi4mJRoVCI3377rSiKonj06FERgLhnzx7DPn/88YcoCIJ4/vx5i8VuCfWPjyiK4tixY8V77rmnwcc40/G5ePGiCEDcunWrKIo39jf1+++/ixKJRMzLyzPss2zZMlGpVIpqtdqyb8AC6h8jURTF22+/XXzmmWcafExTHCO7GnHV1NRg3759SExMNLRJJBIkJiYiNTXVipFZz8mTJxEaGoo2bdpg9OjRyMnJAQDs27cPGo3G6Fi1b98eERERTnussrOzkZeXZ3RMvL29ER8fbzgmqamp8PHxQVxcnGGfxMRESCQSpKWlWTxma9iyZQsCAwPRrl07TJkyBYWFhYY+Zzo+JSUlAAA/Pz8AN/Y3lZqaitjYWAQFBRn2SUpKQmlpKdLT0y0YvWXUP0ZXrFy5Ev7+/ujcuTNmz56NyspKQ19THCO7KrJ76dIl6HQ6ozcMAEFBQTh+/LiVorKe+Ph4rFixAu3atUNubi7mz5+P2267DUeOHEFeXh5cXFzg4+Nj9JigoCDk5eVZJ2Aru/K+zf3+XOnLy8tDYGCgUb9MJoOfn59THLfBgwdj5MiRiIyMRFZWFl588UUMGTIEqampkEqlTnN89Ho9pk+fjn79+qFz584AcEN/U3l5eWZ/v670ORJzxwgAHn30UbRq1QqhoaE4dOgQZs2ahYyMDPz0008AmuYY2VXiImNDhgwx/LtLly6Ij49Hq1at8P3338PNzc2KkZG9evjhhw3/jo2NRZcuXdC2bVts2bIFCQkJVozMspKTk3HkyBGjc8ZkrKFjdPX5ztjYWISEhCAhIQFZWVlo27Ztk7y2XU0V+vv7QyqVmqziyc/PR3BwsJWish0+Pj6IiYlBZmYmgoODUVNTg+LiYqN9nPlYXXnf1/r9CQ4ONlnoo9VqcfnyZac8bm3atIG/vz8yMzMBOMfxmTZtGtatW4fNmzcjLCzM0H4jf1PBwcFmf7+u9DmKho6ROfHx8QBg9Dt0q8fIrhKXi4sLevbsiU2bNhna9Ho9Nm3aBJVKZcXIbEN5eTmysrIQEhKCnj17Qi6XGx2rjIwM5OTkOO2xioyMRHBwsNExKS0tRVpamuGYqFQqFBcXY9++fYZ9UlJSoNfrDX+AzuTcuXMoLCxESEgIAMc+PqIoYtq0afj555+RkpKCyMhIo/4b+ZtSqVQ4fPiwUXLfuHEjlEolOnbsaJk30oyud4zMOXjwIAAY/Q7d8jG6ycUkVvPdd9+JCoVCXLFihXj06FFx0qRJoo+Pj9EKFWfx3HPPiVu2bBGzs7PFHTt2iImJiaK/v7948eJFURRFcfLkyWJERISYkpIi7t27V1SpVKJKpbJy1M2rrKxMPHDggHjgwAERgPjOO++IBw4cEM+cOSOKoii+8cYboo+Pj7h27Vrx0KFD4j333CNGRkaKVVVVhucYPHiw2L17dzEtLU3cvn27GB0dLT7yyCPWektN6lrHp6ysTHz++efF1NRUMTs7W/zrr7/EHj16iNHR0WJ1dbXhORz1+EyZMkX09vYWt2zZIubm5hp+KisrDftc729Kq9WKnTt3FgcNGiQePHhQXL9+vRgQECDOnj3bGm+pyV3vGGVmZoqvvPKKuHfvXjE7O1tcu3at2KZNG3HAgAGG52iKY2R3iUsURfGDDz4QIyIiRBcXF7F3797irl27rB2SVTz00ENiSEiI6OLiIrZs2VJ86KGHxMzMTEN/VVWVOHXqVNHX11d0d3cX77vvPjE3N9eKETe/zZs3iwBMfsaOHSuKYu2S+JdeekkMCgoSFQqFmJCQIGZkZBg9R2FhofjII4+Inp6eolKpFMePHy+WlZVZ4d00vWsdn8rKSnHQoEFiQECAKJfLxVatWokTJ040+VLoqMfH3HEBIH7++eeGfW7kb+r06dPikCFDRDc3N9Hf31987rnnRI1GY+F30zyud4xycnLEAQMGiH5+fqJCoRCjoqLEmTNniiUlJUbPc6vHiLc1ISIiu2JX57iIiIiYuIiIyK4wcRERkV1h4iIiIrvCxEVERHaFiYuIiOwKExcREdkVJi4iIrIrTFxERGRXmLiIiMiuMHEREZFdYeIiIiK78v8uzxXGXTbL/AAAAABJRU5ErkJggg==\n"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"train_path = \"/kaggle/input/crohme-data/val\"\n#print(os.path.join(train_path,\"labels\"))\ncount = 0\nfor files in os.listdir(os.path.join(train_path,\"labels\"))[:10]:\n    #print(files)\n    label_path = train_path+\"/labels/\"+files\n    count+=1\n    #print(label_path)\n    f = open(label_path, 'r')\n    print(f.read())\nprint(count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.486222Z","iopub.execute_input":"2025-04-26T03:44:27.486517Z","iopub.status.idle":"2025-04-26T03:44:27.551997Z","shell.execute_reply.started":"2025-04-26T03:44:27.486491Z","shell.execute_reply":"2025-04-26T03:44:27.551488Z"}},"outputs":[{"name":"stdout","text":"{ 1 \\mbox { u } }\n$77 + 113 \\geq 189$\nx_x^x + y_y^y + z_z^z - x - y - z\n\\log \\Delta\n\\left [ { P } _ { \\mbox { I } } \\right ]\n{ \\sigma = 6 }\n$j^2 = +1$\n$5 / (37 + 36 + 8) = 0.06$\n$\\frac{a}{\\sin A} = \\frac{b}{\\sin B} = \\frac{c}{\\sin C}$\n3 0 \\times 2 9 x^{2 8}\n10\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Set random seeds for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.552710Z","iopub.execute_input":"2025-04-26T03:44:27.553110Z","iopub.status.idle":"2025-04-26T03:44:27.556788Z","shell.execute_reply.started":"2025-04-26T03:44:27.553086Z","shell.execute_reply":"2025-04-26T03:44:27.556152Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Configuration\nclass Config:\n    def __init__(self):\n        # Dataset paths\n        self.data_root = '/kaggle/input/crohme-data'\n        \n        # Model parameters\n        self.embed_dim = 256\n        self.hidden_dim = 512\n        self.num_layers = 1\n        self.dropout = 0.3\n        self.max_seq_len = 150\n        \n        # Training parameters\n        self.batch_size = 32\n        self.num_epochs = 10\n        self.learning_rate = 0.001\n        self.teacher_forcing_ratio = 0.9\n        self.teacher_forcing_decay = 0.9\n        self.grad_clip = 5.0\n        \n        # Tokenizer parameters\n        self.special_tokens = {\n            'PAD': '<PAD>',\n            'START': '<START>',\n            'END': '<END>',\n            'UNK': '<UNK>'\n        }\n        \n        # Image preprocessing\n        self.img_height = 128\n        self.img_width = 512\n        \n        # Checkpoint parameters\n        self.checkpoint_dir = 'checkpoints'\n        self.log_dir = 'logs'\n        \n        # Device\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.557370Z","iopub.execute_input":"2025-04-26T03:44:27.557543Z","iopub.status.idle":"2025-04-26T03:44:27.574393Z","shell.execute_reply.started":"2025-04-26T03:44:27.557528Z","shell.execute_reply":"2025-04-26T03:44:27.573727Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Tokenizer for LaTeX expressions\nclass LaTeXTokenizer:\n    def __init__(self, config):\n        self.config = config\n        self.token2idx = {}\n        self.idx2token = {}\n        self.build_vocab([])  # Initialize with special tokens\n        \n    def build_vocab(self, latex_expressions):\n        # Add special tokens\n        vocab = [self.config.special_tokens[token] for token in ['PAD', 'START', 'END', 'UNK']]\n        \n        # Add all unique tokens from latex expressions\n        all_tokens = []\n        for expr in latex_expressions:\n            tokens = self._tokenize(expr)\n            all_tokens.extend(tokens)\n        \n        # Count token frequencies\n        token_counter = Counter(all_tokens)\n        tokens = [token for token, _ in token_counter.most_common()]\n        \n        # Add tokens to vocabulary that aren't already special tokens\n        for token in tokens:\n            if token not in vocab:\n                vocab.append(token)\n        \n        # Create mappings\n        self.token2idx = {token: idx for idx, token in enumerate(vocab)}\n        self.idx2token = {idx: token for idx, token in enumerate(vocab)}\n        \n        return self\n    \n    def _tokenize(self, latex_str):\n        \"\"\"\n        Tokenize a LaTeX string.\n        This is a simplified approach - in a production system, you might need \n        more sophisticated tokenization based on LaTeX syntax.\n        \"\"\"\n        # Remove extra whitespace\n        latex_str = latex_str.strip()\n        \n        # Handle special LaTeX commands\n        pattern = r'(\\\\[a-zA-Z]+|[^a-zA-Z0-9\\s])'\n        \n        # Split by the pattern but keep the delimiters\n        parts = re.split(f'({pattern})', latex_str)\n        \n        # Filter out empty strings and strip whitespace\n        tokens = [part.strip() for part in parts if part.strip()]\n        \n        return tokens\n    \n    def encode(self, latex_str):\n        \"\"\"Convert LaTeX string to token IDs\"\"\"\n        tokens = self._tokenize(latex_str)\n        \n        # Add START and END tokens\n        tokens = [self.config.special_tokens['START']] + tokens + [self.config.special_tokens['END']]\n        \n        # Convert to indices, using UNK for unknown tokens\n        unk_idx = self.token2idx[self.config.special_tokens['UNK']]\n        indices = [self.token2idx.get(token, unk_idx) for token in tokens]\n        \n        return indices\n    \n    def decode(self, indices):\n        \"\"\"Convert token IDs back to LaTeX string\"\"\"\n        # Convert indices to tokens\n        start_idx = self.token2idx[self.config.special_tokens['START']]\n        end_idx = self.token2idx[self.config.special_tokens['END']]\n        pad_idx = self.token2idx[self.config.special_tokens['PAD']]\n        \n        # Filter out special tokens\n        tokens = []\n        for idx in indices:\n            if idx == end_idx:  # Stop at END token\n                break\n            if idx not in [start_idx, pad_idx]:  # Skip START and PAD tokens\n                tokens.append(self.idx2token[idx])\n        \n        # Join tokens (with space between symbols for readability)\n        latex = ' '.join(tokens)\n        \n        # Clean up spaces around certain symbols\n        latex = re.sub(r'\\s+', ' ', latex)  # Replace multiple spaces with single space\n        for symbol in ['+', '-', '=', '>', '<', '\\\\leq', '\\\\geq']:\n            latex = latex.replace(f' {symbol} ', f' {symbol} ')\n        \n        return latex.strip()\n    \n    @property\n    def vocab_size(self):\n        return len(self.token2idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.575100Z","iopub.execute_input":"2025-04-26T03:44:27.575324Z","iopub.status.idle":"2025-04-26T03:44:27.595264Z","shell.execute_reply.started":"2025-04-26T03:44:27.575309Z","shell.execute_reply":"2025-04-26T03:44:27.594667Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Dataset class for CROHME\nclass CROHMEDataset(Dataset):\n    def __init__(self, data_dir, tokenizer, config, split='train', transform=None):\n        self.data_dir = data_dir\n        self.split = split\n        self.tokenizer = tokenizer\n        self.config = config\n        self.transform = transform if transform else self._get_default_transform()\n        \n        # Get all image paths\n        split_dir = os.path.join(data_dir, split)\n        self.image_paths = sorted(glob.glob(os.path.join(split_dir, 'images', '*.png')))\n        \n        # Load all LaTeX expressions\n        self.latex_expressions = []\n        for img_path in self.image_paths:\n            # Get corresponding label path\n            file_id = os.path.basename(img_path).split('.')[0]\n            label_path = os.path.join(split_dir, 'labels', f\"{file_id}.txt\")\n            \n            if os.path.exists(label_path):\n                with open(label_path, 'r', encoding='utf-8') as f:\n                    latex = f.read().strip()\n                self.latex_expressions.append(latex)\n            else:\n                print(f\"Warning: Label not found for {img_path}\")\n                self.latex_expressions.append(\"\")  # Empty placeholder\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load and transform image\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('L')  # Convert to grayscale\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        # Get and encode LaTeX\n        latex = self.latex_expressions[idx]\n        encoded_latex = self.tokenizer.encode(latex)\n        \n        # Pad sequence if needed\n        if len(encoded_latex) > self.config.max_seq_len:\n            encoded_latex = encoded_latex[:self.config.max_seq_len]\n        else:\n            pad_idx = self.tokenizer.token2idx[self.config.special_tokens['PAD']]\n            encoded_latex = encoded_latex + [pad_idx] * (self.config.max_seq_len - len(encoded_latex))\n        \n        return {\n            'image': image,\n            'latex_tokens': torch.tensor(encoded_latex, dtype=torch.long),\n            'latex_str': latex,\n            'image_path': img_path\n        }\n    \n    def _get_default_transform(self):\n        return transforms.Compose([\n            transforms.Resize((self.config.img_height, self.config.img_width)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5], std=[0.5])\n        ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.595961Z","iopub.execute_input":"2025-04-26T03:44:27.596230Z","iopub.status.idle":"2025-04-26T03:44:27.615605Z","shell.execute_reply.started":"2025-04-26T03:44:27.596182Z","shell.execute_reply":"2025-04-26T03:44:27.614960Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Attention module\nclass AttentionModule(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim):\n        super().__init__()\n        self.encoder_dim = encoder_dim\n        self.decoder_dim = decoder_dim\n        \n        # Attention layers\n        self.attn = nn.Linear(encoder_dim + decoder_dim, decoder_dim)\n        self.v = nn.Linear(decoder_dim, 1, bias=False)\n        \n    def forward(self, encoder_features, decoder_hidden):\n        \"\"\"\n        encoder_features: (batch_size, feature_size, height, width)\n        decoder_hidden: (batch_size, decoder_dim)\n        \"\"\"\n        batch_size = encoder_features.size(0)\n        \n        # Reshape encoder features\n        feature_size = encoder_features.size(1)\n        num_pixels = encoder_features.size(2) * encoder_features.size(3)\n        \n        encoder_features = encoder_features.permute(0, 2, 3, 1).contiguous()\n        encoder_features = encoder_features.view(batch_size, num_pixels, feature_size)\n        \n        # Repeat decoder hidden state\n        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, num_pixels, 1)\n        \n        # Calculate attention weights\n        attention_input = torch.cat((decoder_hidden, encoder_features), dim=2)\n        attention = torch.tanh(self.attn(attention_input))\n        attention = self.v(attention).squeeze(2)\n        \n        # Apply softmax to get attention weights\n        alpha = F.softmax(attention, dim=1)\n        alpha = alpha.unsqueeze(2)\n        \n        # Apply attention weights to encoder features\n        context_vector = (encoder_features * alpha).sum(dim=1)\n        \n        return context_vector, alpha","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.617893Z","iopub.execute_input":"2025-04-26T03:44:27.618061Z","iopub.status.idle":"2025-04-26T03:44:27.632829Z","shell.execute_reply.started":"2025-04-26T03:44:27.618048Z","shell.execute_reply":"2025-04-26T03:44:27.632152Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, encoded_dim):\n        super().__init__()\n        \n        # Use ResNet18 as backbone\n        resnet = models.resnet18(pretrained=True)\n        \n        # Remove final fully connected layer and pooling\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        \n        # Adjust first conv layer to accept grayscale images\n        self.resnet[0] = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        \n        # Add a projection layer to get the desired dimension\n        self.projection = nn.Conv2d(512, encoded_dim, kernel_size=1)\n        \n    def forward(self, images):\n        features = self.resnet(images)\n        features = self.projection(features)\n        return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.633626Z","iopub.execute_input":"2025-04-26T03:44:27.633853Z","iopub.status.idle":"2025-04-26T03:44:27.651552Z","shell.execute_reply.started":"2025-04-26T03:44:27.633838Z","shell.execute_reply":"2025-04-26T03:44:27.650856Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Decoder - LSTM with attention\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, encoder_dim, num_layers=1, dropout=0.5):\n        super().__init__()\n        \n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        self.hidden_dim = hidden_dim\n        self.encoder_dim = encoder_dim\n        self.num_layers = num_layers\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        \n        # Attention module\n        self.attention = AttentionModule(encoder_dim, hidden_dim)\n        \n        # LSTM layer\n        self.lstm = nn.LSTM(\n            input_size=embed_dim + encoder_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        \n        # Output projection\n        self.output = nn.Linear(hidden_dim, vocab_size)\n        \n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward_step(self, encoder_features, prev_token, hidden=None):\n        \"\"\"Single step forward\"\"\"\n        # Get embeddings\n        embed = self.embedding(prev_token)  # (batch_size, 1, embed_dim)\n        \n        batch_size = prev_token.size(0)\n        \n        # Initialize hidden state if None\n        if hidden is None:\n            # Initialize hidden state and cell state as zeros\n            h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(encoder_features.device)\n            c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(encoder_features.device)\n            hidden = (h_0, c_0)\n        \n        # Extract hidden state (ignore cell state for attention)\n        hidden_state = hidden[0][-1]  # Last layer's hidden state\n        \n        # Rest of the method remains the same...\n        # Apply attention\n        context, _ = self.attention(encoder_features, hidden_state)\n        context = context.unsqueeze(1)  # (batch_size, 1, encoder_dim)\n        \n        # Concatenate embedding and context vector\n        lstm_input = torch.cat([embed, context], dim=2)\n        \n        # LSTM step\n        output, hidden = self.lstm(lstm_input, hidden)\n    \n        # Project to vocabulary space\n        output = self.output(self.dropout(output))\n        \n        return output, hidden\n    \n    def forward(self, encoder_features, targets=None, teacher_forcing_ratio=0.5, max_len=None):\n        \"\"\"\n        Forward pass with optional teacher forcing\n        encoder_features: (batch_size, encoder_dim, height, width)\n        targets: (batch_size, max_len) - token indices\n        \"\"\"\n        batch_size = encoder_features.size(0)\n        \n        # Initialize hidden state\n        # Create hidden state and cell state with appropriate dimensions\n        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(encoder_features.device)\n        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(encoder_features.device)\n        hidden = (h_0, c_0)\n        \n        # Determine sequence length\n        if targets is not None:\n            max_len = targets.size(1)\n        \n        # Tensor to store decoder outputs\n        outputs = torch.zeros(batch_size, max_len, self.vocab_size).to(encoder_features.device)\n        \n        # First input is always <START> token\n        start_idx = targets[:, 0] if targets is not None else torch.ones(batch_size).long().to(encoder_features.device)\n        input_token = start_idx.unsqueeze(1)  # (batch_size, 1)\n        \n        # Generate sequence\n        for t in range(max_len):\n            # Forward step\n            output, hidden = self.forward_step(encoder_features, input_token, hidden)\n            \n            # Store output\n            outputs[:, t:t+1, :] = output\n            \n            # Determine next input token\n            use_teacher_forcing = (random.random() < teacher_forcing_ratio) and targets is not None\n            \n            if use_teacher_forcing and t < max_len - 1:\n                # Use ground truth as next input\n                input_token = targets[:, t+1:t+2]\n            else:\n                # Use model's prediction as next input\n                _, top_indices = output.topk(1, dim=2)\n                input_token = top_indices.squeeze(2)\n        \n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.652228Z","iopub.execute_input":"2025-04-26T03:44:27.652435Z","iopub.status.idle":"2025-04-26T03:44:27.670453Z","shell.execute_reply.started":"2025-04-26T03:44:27.652420Z","shell.execute_reply":"2025-04-26T03:44:27.669879Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Complete model combining encoder and decoder\nclass HandwrittenMathRecognizer(nn.Module):\n    def __init__(self, config, vocab_size):\n        super().__init__()\n        self.config = config\n        \n        # Encoder and decoder\n        self.encoder = Encoder(encoded_dim=config.hidden_dim)\n        self.decoder = Decoder(\n            vocab_size=vocab_size,\n            embed_dim=config.embed_dim,\n            hidden_dim=config.hidden_dim,\n            encoder_dim=config.hidden_dim,\n            num_layers=config.num_layers,\n            dropout=config.dropout\n        )\n        \n    def forward(self, images, targets=None, teacher_forcing_ratio=0.5):\n        # Encode images\n        encoder_features = self.encoder(images)\n        \n        # Decode with or without teacher forcing\n        outputs = self.decoder(\n            encoder_features, \n            targets, \n            teacher_forcing_ratio, \n            max_len=self.config.max_seq_len if targets is None else None\n        )\n        \n        return outputs\n    \n    def generate(self, images):\n        \"\"\"Generate LaTeX expressions without teacher forcing\"\"\"\n        with torch.no_grad():\n            encoder_features = self.encoder(images)\n            outputs = self.decoder(\n                encoder_features, \n                targets=None, \n                teacher_forcing_ratio=0.0, \n                max_len=self.config.max_seq_len\n            )\n            \n            # Get predicted tokens\n            _, predicted = outputs.max(2)\n            \n            return predicted","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.671410Z","iopub.execute_input":"2025-04-26T03:44:27.671634Z","iopub.status.idle":"2025-04-26T03:44:27.690914Z","shell.execute_reply.started":"2025-04-26T03:44:27.671619Z","shell.execute_reply":"2025-04-26T03:44:27.690240Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def calculate_metrics(predictions, targets, tokenizer):\n    \"\"\"Calculate evaluation metrics\"\"\"\n    # Convert token indices to LaTeX strings\n    pred_latex = [tokenizer.decode(pred.tolist()) for pred in predictions]\n    target_latex = [tokenizer.decode(target.tolist()) for target in targets]\n    \n    # Calculate exact match accuracy\n    exact_matches = sum(pred == target for pred, target in zip(pred_latex, target_latex))\n    exact_match_accuracy = exact_matches / len(pred_latex)\n    \n    # Calculate token accuracy\n    total_tokens = 0\n    correct_tokens = 0\n    \n    for pred, target in zip(predictions, targets):\n        # Find end index (based on END token or max sequence length)\n        end_idx = config.max_seq_len\n        for i, token in enumerate(target):\n            if token.item() == tokenizer.token2idx[config.special_tokens['END']]:\n                end_idx = i + 1\n                break\n        \n        # Count correct tokens up to end index\n        min_len = min(len(pred), end_idx)\n        total_tokens += end_idx\n        correct_tokens += (pred[:min_len] == target[:min_len]).sum().item()\n    \n    token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0\n    \n    # Calculate simplified BLEU score (character-level n-grams)\n    # This is just a placeholder - you might want to use a proper BLEU implementation\n    bleu_score = 0.0  # Simplified\n    # Generate 5 random indices without replacement\n    total_samples = len(pred_latex)\n    random_indices = np.random.choice(total_samples, size=min(5, total_samples), replace=False)\n\n    \n    return {\n        'exact_match': exact_match_accuracy,\n        'token_accuracy': token_accuracy,\n        'bleu': bleu_score,\n        'pred_examples': [pred_latex[i] for i in random_indices],  # Random sample predictions\n        'target_examples': [target_latex[i] for i in random_indices]  # Corresponding targets\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.691811Z","iopub.execute_input":"2025-04-26T03:44:27.692060Z","iopub.status.idle":"2025-04-26T03:44:27.714863Z","shell.execute_reply.started":"2025-04-26T03:44:27.692028Z","shell.execute_reply":"2025-04-26T03:44:27.714102Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def train_epoch(model, dataloader, criterion, optimizer, config, epoch):\n    model.train()\n    total_loss = 0\n\n    # teacher forcing ratio with decay\n    teacher_forcing_ratio = config.teacher_forcing_ratio * (config.teacher_forcing_decay ** epoch)\n    \n    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1} Training\")\n    for batch in progress_bar:\n        images = batch['image'].to(config.device)\n        targets = batch['latex_tokens'].to(config.device)\n\n        # forward\n        outputs = model(images, targets, teacher_forcing_ratio)\n\n        # flatten for loss\n        outputs_flat = outputs.contiguous().view(-1, outputs.size(-1))\n        targets_flat = targets.contiguous().view(-1)\n\n        # compute loss (ignore_index already set in criterion)\n        loss = criterion(outputs_flat, targets_flat)\n\n        # backward + optimize\n        optimizer.zero_grad()\n        loss.backward()\n        if config.grad_clip > 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n        optimizer.step()\n\n        total_loss += loss.item()\n        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", tf_ratio=f\"{teacher_forcing_ratio:.2f}\")\n\n    return total_loss / len(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.715603Z","iopub.execute_input":"2025-04-26T03:44:27.716098Z","iopub.status.idle":"2025-04-26T03:44:27.733519Z","shell.execute_reply.started":"2025-04-26T03:44:27.716073Z","shell.execute_reply":"2025-04-26T03:44:27.732719Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def evaluate(model, dataloader, criterion, tokenizer, config):\n    model.eval()\n    total_loss = 0\n    all_predictions = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            # Get batch data\n            images = batch['image'].to(config.device)\n            targets = batch['latex_tokens'].to(config.device)\n            \n            # Forward pass for loss calculation\n            outputs = model(images, targets, teacher_forcing_ratio=0.0)\n            \n            # Reshape for loss calculation\n            outputs_flat = outputs.contiguous().view(-1, outputs.size(-1))\n            targets_flat = targets.contiguous().view(-1)\n            \n            # Calculate loss\n            loss = criterion(outputs_flat, targets_flat)\n            total_loss += loss.item()\n            \n            # Generate predictions for accuracy calculation\n            predictions = model.generate(images)\n            \n            # Store predictions and targets\n            all_predictions.extend(predictions.detach().cpu())\n            all_targets.extend(targets.detach().cpu())\n    \n    # Calculate metrics\n    metrics = calculate_metrics(all_predictions, all_targets, tokenizer)\n    metrics['loss'] = total_loss / len(dataloader)\n    \n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.734146Z","iopub.execute_input":"2025-04-26T03:44:27.734357Z","iopub.status.idle":"2025-04-26T03:44:27.752672Z","shell.execute_reply.started":"2025-04-26T03:44:27.734343Z","shell.execute_reply":"2025-04-26T03:44:27.751982Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Create datasets\n#train_dataset = CROHMEDataset('/kaggle/input/crohme-data', tokenizer, config, split='train')\n#val_dataset = CROHMEDataset(config.data_root, tokenizer, config, split='val')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.753387Z","iopub.execute_input":"2025-04-26T03:44:27.753585Z","iopub.status.idle":"2025-04-26T03:44:27.772839Z","shell.execute_reply.started":"2025-04-26T03:44:27.753570Z","shell.execute_reply":"2025-04-26T03:44:27.772151Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from torch.optim.lr_scheduler import ReduceLROnPlateau\n\ndef train_model(config):\n    \"\"\"Main training function\"\"\"\n    print(f\"Using device: {config.device}\")\n    \n    # Create checkpoint directory if it doesn't exist\n    os.makedirs(config.checkpoint_dir, exist_ok=True)\n    os.makedirs(config.log_dir, exist_ok=True)\n    \n    # Load dataset\n    print(\"Loading dataset...\")\n    \n    # First, get all LaTeX expressions to build vocabulary\n    all_latex = []\n    for split in ['train', 'val']:\n        split_dir = os.path.join(config.data_root, split)\n        label_files = glob.glob(os.path.join(split_dir, 'labels', '*.txt'))\n        \n        for label_file in tqdm(label_files, desc=f\"Reading {split} labels\"):\n            with open(label_file, 'r', encoding='utf-8') as f:\n                all_latex.append(f.read().strip())\n    \n    # Create tokenizer and build vocabulary\n    print(\"Building vocabulary...\")\n    tokenizer = LaTeXTokenizer(config)\n    tokenizer.build_vocab(all_latex)\n    print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n    \n    # Create datasets\n    train_dataset = CROHMEDataset(config.data_root, tokenizer, config, split='train')\n    val_dataset = CROHMEDataset(config.data_root, tokenizer, config, split='val')\n    \n    print(f\"Train dataset size: {len(train_dataset)}\")\n    print(f\"Validation dataset size: {len(val_dataset)}\")\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=config.batch_size, \n        shuffle=True, \n        pin_memory=True,\n        num_workers=2\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=config.batch_size, \n        shuffle=False, \n        pin_memory=True,\n        num_workers=2\n    )\n    \n    # Create model\n    print(\"Creating model...\")\n    model = HandwrittenMathRecognizer(config, tokenizer.vocab_size).to(config.device)\n    \n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token2idx[config.special_tokens['PAD']])\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n    )\n    \n    # Training loop\n    print(f\"Starting training for {config.num_epochs} epochs...\")\n    best_val_loss = float('inf')\n    \n    for epoch in range(config.num_epochs):\n        # Train\n        train_loss = train_epoch(model, train_loader, criterion, optimizer, config, epoch)\n        train_metrics = evaluate(model, train_loader, criterion, tokenizer, config)\n        # Evaluate\n        val_metrics = evaluate(model, val_loader, criterion, tokenizer, config)\n        val_loss = val_metrics['loss']\n        \n        # Update learning rate\n        scheduler.step(val_loss)\n        \n        # Print metrics\n        print(f\"Epoch {epoch+1}/{config.num_epochs}:\")\n        print(f\"  Train Loss: {train_loss:.4f}\")\n        print(f\"  Val Loss: {val_loss:.4f}\")\n        print(f\"  Exact Match: {val_metrics['exact_match']:.4f}\")\n        print(f\"  Token Accuracy: {val_metrics['token_accuracy']:.4f}\")\n        \n        # Sample predictions\n        print(\"Sample predictions:\")\n        for i in range(min(3, len(val_metrics['pred_examples']))):\n            print(f\"  Pred: {val_metrics['pred_examples'][i]}\")\n            print(f\"  True: {val_metrics['target_examples'][i]}\")\n            print()\n        \n        # Save checkpoint if improved\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            checkpoint_path = os.path.join(config.checkpoint_dir, 'best_model.pth')\n            \n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'val_metrics': val_metrics,\n                'tokenizer': tokenizer,\n                'config': config\n            }, checkpoint_path)\n            \n            print(f\"Saved best model checkpoint to {checkpoint_path}\")\n        \n        # Always save latest model\n        checkpoint_path = os.path.join(config.checkpoint_dir, 'latest_model.pth')\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'val_metrics': val_metrics,\n            'tokenizer': tokenizer,\n            'config': config\n        }, checkpoint_path)\n    \n    print(\"Training complete!\")\n    return model, tokenizer, val_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.773548Z","iopub.execute_input":"2025-04-26T03:44:27.773749Z","iopub.status.idle":"2025-04-26T03:44:27.790665Z","shell.execute_reply.started":"2025-04-26T03:44:27.773731Z","shell.execute_reply":"2025-04-26T03:44:27.790126Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def test_model(model, tokenizer, config, test_loader=None):\n    \"\"\"Test the model on the test set\"\"\"\n    if test_loader is None:\n        # Create test dataset and loader\n        test_dataset = CROHMEDataset(config.data_root, tokenizer, config, split='test')\n        test_loader = DataLoader(\n            test_dataset, \n            batch_size=config.batch_size, \n            shuffle=False,\n            pin_memory=True\n        )\n    \n    # Evaluate\n    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token2idx[config.special_tokens['PAD']])\n    metrics = evaluate(model, test_loader, criterion, tokenizer, config)\n    \n    print(\"Test Results:\")\n    print(f\"  Loss: {metrics['loss']:.4f}\")\n    print(f\"  Exact Match: {metrics['exact_match']:.4f}\")\n    print(f\"  Token Accuracy: {metrics['token_accuracy']:.4f}\")\n    \n    # Sample predictions\n    print(\"Sample predictions:\")\n    for i in range(min(5, len(metrics['pred_examples']))):\n        print(f\"  Pred: {metrics['pred_examples'][i]}\")\n        print(f\"  True: {metrics['target_examples'][i]}\")\n        print()\n    \n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.791343Z","iopub.execute_input":"2025-04-26T03:44:27.791569Z","iopub.status.idle":"2025-04-26T03:44:27.810094Z","shell.execute_reply.started":"2025-04-26T03:44:27.791549Z","shell.execute_reply":"2025-04-26T03:44:27.809607Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def predict_image(model, tokenizer, image_path, config):\n    \"\"\"Predict LaTeX for a single image\"\"\"\n    # Load and preprocess image\n    transform = transforms.Compose([\n        transforms.Resize((config.img_height, config.img_width)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5], std=[0.5])\n    ])\n    \n    image = Image.open(image_path).convert('L')\n    image = transform(image).unsqueeze(0).to(config.device)\n    \n    # Generate prediction\n    model.eval()\n    with torch.no_grad():\n        prediction = model.generate(image)\n        latex = tokenizer.decode(prediction[0].tolist())\n    \n    return latex","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.810721Z","iopub.execute_input":"2025-04-26T03:44:27.810903Z","iopub.status.idle":"2025-04-26T03:44:27.831037Z","shell.execute_reply.started":"2025-04-26T03:44:27.810889Z","shell.execute_reply":"2025-04-26T03:44:27.830557Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def load_checkpoint(checkpoint_path, device=None):\n    \"\"\"Load model from checkpoint\"\"\"\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    \n    # Get config and tokenizer\n    config = checkpoint['config']\n    tokenizer = checkpoint['tokenizer']\n    \n    # Create model\n    model = HandwrittenMathRecognizer(config, tokenizer.vocab_size).to(device)\n    \n    # Load weights\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    return model, tokenizer, config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.831726Z","iopub.execute_input":"2025-04-26T03:44:27.832101Z","iopub.status.idle":"2025-04-26T03:44:27.846841Z","shell.execute_reply.started":"2025-04-26T03:44:27.832086Z","shell.execute_reply":"2025-04-26T03:44:27.846112Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Instead of argparse, define these variables directly\nmode = 'train'  # Options: 'train', 'test', 'predict'\ndata_root = '/kaggle/input/crohme-data'\n# /kaggle/input/crohme-data-basic/filtered_basic_arithmetic\ncheckpoint_path = None  # Path to checkpoint if needed\nimage_path = None  # Path to image for prediction\nbatch_size = 32\nnum_epochs = 50\nlearning_rate = 0.001\n\n# Set up configuration\nconfig = Config()\nconfig.data_root = data_root\nconfig.batch_size = batch_size\nconfig.num_epochs = num_epochs\nconfig.learning_rate = learning_rate\n\n# Set seed for reproducibility\nset_seed()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.847575Z","iopub.execute_input":"2025-04-26T03:44:27.847879Z","iopub.status.idle":"2025-04-26T03:44:27.953727Z","shell.execute_reply.started":"2025-04-26T03:44:27.847864Z","shell.execute_reply":"2025-04-26T03:44:27.953118Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Training mode\nif mode == 'train':\n    if checkpoint_path:\n        # Continue training from checkpoint\n        print(f\"Loading checkpoint from {checkpoint_path}...\")\n        model, tokenizer, loaded_config = load_checkpoint(checkpoint_path)\n        \n        # Update config with loaded config\n        for key, value in vars(loaded_config).items():\n            if key not in ['batch_size', 'num_epochs', 'learning_rate']:\n                setattr(config, key, value)\n        \n        model, tokenizer, val_metrics = train_model(config)\n    else:\n        # Train from scratch\n        model, tokenizer,val_metrics = train_model(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.954795Z","iopub.execute_input":"2025-04-26T03:44:27.955420Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading dataset...\n","output_type":"stream"},{"name":"stderr","text":"Reading train labels: 100%|| 9999/9999 [00:54<00:00, 183.97it/s]\nReading val labels: 100%|| 1983/1983 [00:10<00:00, 189.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Building vocabulary...\nVocabulary size: 1480\nTrain dataset size: 9999\nValidation dataset size: 1983\nCreating model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|| 44.7M/44.7M [00:00<00:00, 183MB/s] \n/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Starting training for 50 epochs...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1 Training: 100%|| 313/313 [02:58<00:00,  1.75it/s, loss=0.4002, tf_ratio=0.90]\nEvaluating: 100%|| 62/62 [00:25<00:00,  2.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50:\n  Train Loss: 0.7427\n  Val Loss: 6.1559\n  Exact Match: 0.0000\n  Token Accuracy: 0.0989\nSample predictions:\n  Pred: $ $ $ $ $ $ $ $ $ $ $ $\n  True: $ $ \\lim \\lim _ _ { { x \\rightarrow \\rightarrow + + \\infty \\infty } } \\frac \\frac { { e ^ ^ { { x } } } } { { x } } $ $\n\n  Pred: $ $ $ $ $ ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n  True: { { 6 . . 3 \\int \\int \\mbox \\mbox { { m } } dr } }\n\n  Pred: $ $ $ $ $ ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n  True: \\log \\log _ _ 2 \\frac \\frac 1 2 + + \\log \\log _ _ 4 \\frac \\frac 2 4\n\nSaved best model checkpoint to checkpoints/best_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|| 313/313 [03:06<00:00,  1.68it/s, loss=0.3823, tf_ratio=0.81]\nEvaluating: 100%|| 62/62 [00:25<00:00,  2.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/50:\n  Train Loss: 0.4698\n  Val Loss: 5.0967\n  Exact Match: 0.0000\n  Token Accuracy: 0.1472\nSample predictions:\n  Pred: $ $ \\frac \\frac \\frac { { { \\frac \\frac \\frac { { 1 1 } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } }\n  True: \\frac \\frac { { \\sin \\sin B + + \\sin \\sin C } } { { \\cos \\cos B + + \\cos \\cos C } }\n\n  Pred: $ $ $ $ $\n  True: { { n } } ^ ^ { { { { - - k } } } }\n\n  Pred: $ $ $\n  True: z\n\nSaved best model checkpoint to checkpoints/best_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|| 313/313 [03:06<00:00,  1.68it/s, loss=0.4601, tf_ratio=0.73]\nEvaluating: 100%|| 62/62 [00:25<00:00,  2.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/50:\n  Train Loss: 0.5976\n  Val Loss: 4.3836\n  Exact Match: 0.0000\n  Token Accuracy: 0.2012\nSample predictions:\n  Pred: $ $ \\frac \\frac { { { 1 } } { { { } } } } } { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { {\n  True: $ $ \\cos \\cos ^ ^ { { n } } \\theta \\theta $ $\n\n  Pred: $ $ \\frac \\frac { { { 1 } } { { { } } } } } } } { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { {\n  True: A = = \\sqrt \\sqrt { { a + + \\frac \\frac 1 { { \\sqrt \\sqrt { { a + + \\frac \\frac 1 { { \\sqrt \\sqrt a } } } } } } } } + + \\sqrt \\sqrt b\n\n  Pred: $ $ \\frac \\frac \\frac { { { 1 } } { { { } } } } } } { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { {\n  True: $ $ f ( ( x ) ) = = \\frac \\frac { { x ^ ^ 2 - - 4 } } { { 4x ^ ^ 2 } } \\frac \\frac { { 1 } } { { x - - 2 } } = = \\frac \\frac { { x + + 2 } } { { 4x ^ ^ 2 } } $ $\n\nSaved best model checkpoint to checkpoints/best_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 Training: 100%|| 313/313 [03:06<00:00,  1.68it/s, loss=0.4973, tf_ratio=0.66]\nEvaluating: 100%|| 62/62 [00:25<00:00,  2.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/50:\n  Train Loss: 0.6917\n  Val Loss: 4.5542\n  Exact Match: 0.0015\n  Token Accuracy: 0.1781\nSample predictions:\n  Pred: $ $ x = = = = = \\frac \\frac { { { ( ( ( - - - 1 ) ) ) ) } } } { { { } } } $ $ $\n  True: $ $ 3n ^ ^ 2 + + 2n $ $\n\n  Pred: $ $ \\frac \\frac { { { ( ( ( \\frac \\frac \\frac { { \\pi \\pi \\pi } } } { { \\cos \\cos \\cos } } } $ $ $ $ $\n  True: $ $ x = = \\frac \\frac { { m _ _ { { eau } } } } { { m _ _ { { air sec } } } } $ $\n\n  Pred: $ $ ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n  True: $ $ ( ( ( ( 15 \\times \\times 131 ) ) / / 116 ) ) / / ( ( ( ( 75 \\times \\times 22 ) ) - - 169 ) ) \\neq \\neq 0 $ $\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5 Training: 100%|| 313/313 [03:06<00:00,  1.68it/s, loss=1.0683, tf_ratio=0.59]\nEvaluating: 100%|| 62/62 [00:25<00:00,  2.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/50:\n  Train Loss: 0.7811\n  Val Loss: 3.9359\n  Exact Match: 0.0000\n  Token Accuracy: 0.2099\nSample predictions:\n  Pred: $ $ ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n  True: $ $ ( ( 51 - - 53 + + 31 ) ) - - ( ( ( ( 186 \\times \\times 21 ) ) \\times \\times ( ( 161 \\div \\div 103 ) ) ) ) \\neq \\neq - - 8248 $ $\n\n  Pred: $ $ x = = = \\frac \\frac { { { } } } { { { { { { { { { { { { { { { { { { { } } } } } } } } } } } } } } } { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { {\n  True: $ $ S = = \\ \\ { { 1 , , s , , s ^ ^ 2 , , s ^ ^ 3 . . . . . . . . . . . . . . \\ \\ } } $ $\n\n  Pred: $ $ ( ( ( ( ( ( ( ( ( ( ( ( ) ) ) ) ) ) ) $ $ $\n  True: $ $ ( ( e _ _ 1 , , \\ldots \\ldots , , e _ _ n ) ) $ $\n\nSaved best model checkpoint to checkpoints/best_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 Training: 100%|| 313/313 [03:04<00:00,  1.70it/s, loss=1.1305, tf_ratio=0.53]\nEvaluating: 100%|| 62/62 [00:25<00:00,  2.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/50:\n  Train Loss: 0.8936\n  Val Loss: 3.9776\n  Exact Match: 0.0000\n  Token Accuracy: 0.2108\nSample predictions:\n  Pred: $ $ \\frac \\frac { { { } } { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { {\n  True: $ $ \\frac \\frac { { V _ _ ts } } { { V _ _ m } } $ $\n\n  Pred: \\lim \\lim _ _ { { x \\rightarrow \\rightarrow \\frac \\frac { { 1 } } { { { } } } } } \\frac \\frac { { 1 } } { { 4 } } } } } } { { 1 1 1 } } { { 1 } } } } } { { { 1 - - 4 x 4 x } } }\n  True: \\sin \\sin \\left \\left ( ( \\frac \\frac { { \\pi \\pi } } { { 4 } } \\right \\right ) ) = = \\frac \\frac { { 1 } } { { \\sqrt \\sqrt { { 2 } } } }\n\n  Pred: $ $ ( ( ( ( - - - 1 ) ) ) ) = = = = = 0 0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . RC $ $\n  True: $ $ 15 \\pm \\pm ( ( 12 - - 10 - - 29 ) ) \\div \\div 161 $ $\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7 Training: 100%|| 313/313 [03:03<00:00,  1.70it/s, loss=0.7736, tf_ratio=0.48]\nEvaluating: 100%|| 62/62 [00:25<00:00,  2.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/50:\n  Train Loss: 0.9873\n  Val Loss: 3.5618\n  Exact Match: 0.0061\n  Token Accuracy: 0.2648\nSample predictions:\n  Pred: $ $ n _ _ { { $ $ $\n  True: { { 8 \\sum \\sum { { 2 } } } }\n\n  Pred: $ $ n = = 1 $ $\n  True: $ $ u \\times \\times v $ $\n\n  Pred: \\sum \\sum { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } { { { { { { { { { { { { { { { { { { { { { { { } } } } } } } }\n  True: \\sum \\sum _ _ { { { { R \\geq \\geq H } } } } { { B } }\n\nSaved best model checkpoint to checkpoints/best_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8 Training: 100%|| 313/313 [03:04<00:00,  1.70it/s, loss=0.6819, tf_ratio=0.43]\nEvaluating: 100%|| 62/62 [00:25<00:00,  2.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/50:\n  Train Loss: 1.0686\n  Val Loss: 3.3548\n  Exact Match: 0.0005\n  Token Accuracy: 0.2694\nSample predictions:\n  Pred: $ $ ( ( ( ( ( ( ( ) ) ) ) ) = = ( ( ( ( ( ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ( ( ( ( ( ( ( ( ( ) ) ) ) ) ) ) ) ) ) ) ) $ $\n  True: $ $ ( ( ( ( 15 \\times \\times 131 ) ) / / 116 ) ) / / ( ( ( ( 75 \\times \\times 22 ) ) - - 169 ) ) \\neq \\neq 0 $ $\n\n  Pred: $ $ { { ^ ^ ^ { } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } }\n  True: \\mbox \\mbox { { izu } }\n\n  Pred: \\lim \\lim _ _ { { x \\rightarrow \\rightarrow \\frac \\frac { { 1 } } { { 4 } } } } \\frac \\frac { { 1 } } { { 4 } } } } } } { { 1 1 - - - ^ ^ ^ { { x } } } } } { { 1 - - 1 } } } } { { 1 - - - } } }\n  True: \\lim \\lim _ _ { { x \\rightarrow \\rightarrow - - 1 } } \\frac \\frac { { x ^ ^ { { 3 } } + + 1 } } { { x + + 1 } }\n\nSaved best model checkpoint to checkpoints/best_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9 Training: 100%|| 313/313 [03:02<00:00,  1.71it/s, loss=0.6608, tf_ratio=0.39]\nEvaluating: 100%|| 62/62 [00:25<00:00,  2.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/50:\n  Train Loss: 1.1531\n  Val Loss: 3.4160\n  Exact Match: 0.0358\n  Token Accuracy: 0.3144\nSample predictions:\n  Pred: \\frac \\frac { { 1 } } { { 2 } } \\pi \\pi \\pi ^ ^ 2 h 2 h\n  True: \\frac \\frac { { 1 } } { { a } } F \\left \\left ( ( a x + + b \\right \\right ) ) + + C\n\n  Pred: \\log \\log _ _ { { 2 } } 8 + + \\log \\log _ _ { { 2 } } 8 + + \\log \\log _ _ { { 2 } } 8 + + \\log \\log _ _ { { 2 } } 8 + + \\log \\log _ _ { { 2 } } 1 6\n  True: \\sin \\sin ^ ^ 2 ( ( x ) ) + + \\cos \\cos ^ ^ 2 ( ( x ) ) = = 1\n\n  Pred: x _ _ 1 { { 1 1 } } { { 1 } } + + \\frac \\frac \\frac { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { { } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } }\n  True: 1 + + \\frac \\frac { { 1 } } { { 1 ! ! } } + + \\frac \\frac { { 1 } } { { 2 ! ! } } + + \\frac \\frac { { 1 } } { { 3 ! ! } } + + \\frac \\frac { { 1 } } { { 4 ! ! } }\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10 Training: 100%|| 313/313 [03:03<00:00,  1.71it/s, loss=1.3889, tf_ratio=0.35]\nEvaluating: 100%|| 62/62 [00:25<00:00,  2.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/50:\n  Train Loss: 1.1980\n  Val Loss: 3.0673\n  Exact Match: 0.0197\n  Token Accuracy: 0.3031\nSample predictions:\n  Pred: $ $ \\cos \\cos ( ( x ) ) ) = = = = = a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n  True: \\log \\log _ _ { { 2 } } 8 + + \\log \\log _ _ { { 3 } } 9 + + \\log \\log _ _ { { 4 } } 1 6\n\n  Pred: $ $ { { { { { { { { } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } }\n  True: \\frac \\frac { { \\mbox \\mbox { { n } } } } { { \\sqrt \\sqrt { { Y } } } }\n\n  Pred: $ $ a _ _ { { = = = = = = = = _ _ _ { { n n _ _ { { } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } }\n  True: $ $ P _ _ f = = p _ _ c \\times \\times ( ( 1 - - o _ _ i - - o _ _ t ) ) + + o _ _ t $ $\n\nSaved best model checkpoint to checkpoints/best_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11 Training: 100%|| 313/313 [03:03<00:00,  1.70it/s, loss=1.0535, tf_ratio=0.31]\nEvaluating: 100%|| 62/62 [00:25<00:00,  2.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/50:\n  Train Loss: 1.2485\n  Val Loss: 2.6764\n  Exact Match: 0.0499\n  Token Accuracy: 0.3920\nSample predictions:\n  Pred: $ $ a + + c = = 0 $ $\n  True: $ $ bz + + d = = 0 $ $\n\n  Pred: $ $ ( ( { { { } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } }\n  True: \\int \\int { { \\mbox \\mbox { { I } } + + \\mbox \\mbox { { n } } } } dY\n\n  Pred: $ $ x ^ ^ 2 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n  True: $ $ x ^ ^ { { 11 } } - - x ^ ^ { { 10 } } - - x ^ ^ 8 - - x ^ ^ 6 - - x ^ ^ 4 - - x ^ ^ 2 - - 1 $ $\n\nSaved best model checkpoint to checkpoints/best_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12 Training: 100%|| 313/313 [03:02<00:00,  1.72it/s, loss=0.9015, tf_ratio=0.28]\nEvaluating: 100%|| 62/62 [00:25<00:00,  2.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/50:\n  Train Loss: 1.2753\n  Val Loss: 2.4906\n  Exact Match: 0.1125\n  Token Accuracy: 0.4502\nSample predictions:\n  Pred: \\mbox \\mbox { { j } }\n  True: G\n\n  Pred: \\left \\left ( ( x ^ ^ { { 3 } } - - x ^ ^ { { 2 } } - - x \\right \\right ) ) \\left \\left ( ( 2 x 2 x - - 7 \\right \\right ) ) )\n  True: \\left \\left ( ( x ^ ^ { { 3 } } - - x ^ ^ { { 2 } } - - x \\right \\right ) ) \\left \\left ( ( 2 x - - 7 \\right \\right ) )\n\n  Pred: \\sqrt \\sqrt { { \\mbox \\mbox { { } } } } }\n  True: \\sqrt \\sqrt { { a } }\n\nSaved best model checkpoint to checkpoints/best_model.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13 Training:  81%|  | 254/313 [02:29<00:34,  1.72it/s, loss=1.0029, tf_ratio=0.25]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"mode = 'test'  # Options: 'train', 'test', 'predict'\ndata_root = '/kaggle/input/crohme-data'\ncheckpoint_path = \"checkpoints/best_model.pth\"\n# Testing mode\nif mode == 'test':\n    if not checkpoint_path:\n        print(\"Error: Checkpoint path is required for testing\")\n    else:\n        # Load model from checkpoint\n        model, tokenizer, loaded_config = load_checkpoint(checkpoint_path)\n        \n        # Update config with loaded config\n        for key, value in vars(loaded_config).items():\n            setattr(config, key, value)\n        \n        # Test model\n        metrics = test_model(model, tokenizer, config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mode = 'predict'  # Options: 'train', 'test', 'predict'\ndata_root = '/kaggle/input/crohme-data'\ncheckpoint_path = \"checkpoints/best_model.pth\"\nimage_path = \"/kaggle/input/crohme-data/val/images/expr_000541.png\"\n# Prediction mode\nif mode == 'predict':\n    if not checkpoint_path or not image_path:\n        print(\"Error: Both checkpoint and image paths are required for prediction\")\n    else:\n        # Load model from checkpoint\n        model, tokenizer, loaded_config = load_checkpoint(checkpoint_path)\n        \n        # Update config with loaded config\n        for key, value in vars(loaded_config).items():\n            setattr(config, key, value)\n        \n        # Predict on image\n        latex = predict_image(model, tokenizer, image_path, config)\n        print(f\"Predicted LaTeX: {latex}\")\n        \n        # Optional: Display the image\n        from PIL import Image\n        import matplotlib.pyplot as plt\n        \n        plt.figure(figsize=(10, 4))\n        plt.imshow(Image.open(image_path).convert('L'), cmap='gray')\n        plt.title(f\"Prediction: {latex}\")\n        plt.axis('off')\n        plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}